{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import models\n",
    "import lib\n",
    "import resources\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from my_classes import DataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications import vgg16, resnet50, inception_v3\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding, Flatten, Lambda\n",
    "from keras import layers, models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    \"\"\"\n",
    "    Downloads raw data needed and extracts data for all 3 models.\n",
    "    Image - Convert mp4 files into a series of jpeg images\n",
    "    Audio - Extract mp3 files from each mp4 file\n",
    "    Text - Extract text from annotation files\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract images, audio files, and text transcripts for each partition\n",
    "    for partition in ['training', 'test', 'validation']:\n",
    "\n",
    "        # Chop video up into images and save into separate directory\n",
    "        lib.extract_images(partition, num_frames=10)\n",
    "\n",
    "        # Strip audio from mp4 and save in separate directory\n",
    "        lib.extract_audio(partition)\n",
    "\n",
    "        # Take text from transcripts\n",
    "        lib.extract_text(partition)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform():\n",
    "    \"\"\"\n",
    "    Transforms all features for the 3 models.\n",
    "    Image - Convert jpegs to numpy arrays and preprocess for the vgg16 model\n",
    "    Audio - Use librosa to extract features and save dataframe with all features for each video\n",
    "    Text - Tokenize, and convert to indices based on the google news 20 word embeddings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_matrix, word_to_index = resources.create_embedding_matrix()\n",
    "    #for the base case, do for all partitions\n",
    "    for partition in ['training', 'test','validation']:#, 'validation']:\n",
    "\n",
    "        # Transform raw jpegs into numpy arrays\n",
    "        #lib.transform_images(partition=partition, num_frames=10)\n",
    "\n",
    "        # Transform raw audio to feature matrix\n",
    "        #lib.transform_audio(partition=partition, n_mfcc=13)\n",
    "\n",
    "        # Transform text to tokens\n",
    "        lib.transform_text(partition=partition, word_to_index=word_to_index)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (saving only the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_lrcn(l_phase=1):\n",
    "    \"\"\"\n",
    "    Model that takes in still frames of the video and uses vgg16 to eaxtract features from image.\n",
    "    Those features are then fed to a lstm to understand the temporal aspect of the videos.\n",
    "    :return: keras model object\n",
    "    \"\"\"\n",
    "\n",
    "    # Set learning phase to 0\n",
    "    K.set_learning_phase(l_phase)\n",
    "\n",
    "    # Set input layer\n",
    "    video = layers.Input(shape=(None, 224, 224, 3), name='video_input')\n",
    "\n",
    "    # Load the VGG16 model\n",
    "    cnn = vgg16.VGG16(weights=\"imagenet\", include_top=False, pooling='max')\n",
    "    cnn.trainable = True\n",
    "\n",
    "    # Wrap cnn into Lambda and pass it into TimeDistributed\n",
    "    encoded_frame = layers.TimeDistributed(Lambda(lambda x: cnn(x)))(video)\n",
    "    encoded_vid = layers.LSTM(64)(encoded_frame)\n",
    "    encoded_vid = layers.Dropout(.05)(encoded_vid)\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.0005, decay=0.001)\n",
    "    outputs = layers.Dense(6, activation='relu')(encoded_vid)\n",
    "    model = models.Model(inputs=[video], outputs=outputs)\n",
    "    model.compile(optimizer=adam_opt, loss='mean_squared_error')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_lrcn_resnet50():\n",
    "    \"\"\"\n",
    "    Model that takes in still frames of the video and uses vgg16 to eaxtract features from image.\n",
    "    Those features are then fed to a lstm to understand the temporal aspect of the videos.\n",
    "    :return: keras model object\n",
    "    \"\"\"\n",
    "\n",
    "    # Set learning phase to 0\n",
    "    #K.set_learning_phase(0)\n",
    "\n",
    "    # Set input layer\n",
    "    video = layers.Input(shape=(None, 224, 224, 3), name='video_input')\n",
    "\n",
    "    # Load the VGG16 model\n",
    "    cnn = resnet50.ResNet50(weights=\"imagenet\", include_top=False, pooling='max')\n",
    "    cnn.trainable = True\n",
    "\n",
    "    # Wrap cnn into Lambda and pass it into TimeDistributed\n",
    "    encoded_frame = layers.TimeDistributed(Lambda(lambda x: cnn(x)))(video)\n",
    "    encoded_vid = layers.LSTM(64)(encoded_frame)\n",
    "    encoded_vid = layers.Dropout(.05)(encoded_vid)\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.0005, decay=0.001)\n",
    "    outputs = layers.Dense(6, activation='relu')(encoded_vid)\n",
    "    model = models.Model(inputs=[video], outputs=outputs)\n",
    "    model.compile(optimizer=adam_opt, loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_lrcn_inceptionv3():\n",
    "    \"\"\"\n",
    "    Model that takes in still frames of the video and uses vgg16 to eaxtract features from image.\n",
    "    Those features are then fed to a lstm to understand the temporal aspect of the videos.\n",
    "    :return: keras model object\n",
    "    \"\"\"\n",
    "\n",
    "    # Set learning phase to 0\n",
    "    #K.set_learning_phase(0)\n",
    "\n",
    "    # Set input layer\n",
    "    video = layers.Input(shape=(None, 224, 224, 3), name='video_input')\n",
    "\n",
    "    # Load the VGG16 model\n",
    "    cnn = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False, pooling='max')\n",
    "    cnn.trainable = True\n",
    "\n",
    "    # Wrap cnn into Lambda and pass it into TimeDistributed\n",
    "    encoded_frame = layers.TimeDistributed(Lambda(lambda x: cnn(x)))(video)\n",
    "    encoded_vid = layers.LSTM(64)(encoded_frame)\n",
    "    encoded_vid = layers.Dropout(.05)(encoded_vid)\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.0005, decay=0.001)\n",
    "    outputs = layers.Dense(1, activation='linear')(encoded_vid)\n",
    "    model = models.Model(inputs=[video], outputs=outputs)\n",
    "    model.compile(optimizer=adam_opt, loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lstm_model(embedding_matrix):\n",
    "    \"\"\"\n",
    "    Generate a convolutional neural network model, with an embedding layer.\n",
    "    :param embedding_matrix: An embedding matrix, with shape (n,m), where n is the number of words, and m is the\n",
    "    dimensionality of the embedding\n",
    "    :return: keras model object\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of words in the word lookup index\n",
    "    embedding_input_dim = embedding_matrix.shape[0]\n",
    "\n",
    "    # Number of dimensions in the embedding\n",
    "    embedding_output_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    # Maximum length of the x vectors\n",
    "    embedding_input_length = 80\n",
    "\n",
    "    print('embedding_input_dim: {}, embedding_output_dim: {}, embedding_input_length: {}'\n",
    "                 .format(embedding_input_dim, embedding_output_dim, embedding_input_length))\n",
    "\n",
    "    # Define model architecture\n",
    "    embedding_layer = Embedding(input_dim=embedding_input_dim,\n",
    "                                output_dim=embedding_output_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=embedding_input_length,\n",
    "                                trainable=False)\n",
    "    sequence_input = keras.Input(shape=(embedding_input_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Dropout(.5)(embedded_sequences)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    preds = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "    # Compile architecture\n",
    "    text_model = Model(sequence_input, preds)\n",
    "    text_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(image=True, audio=False, text=False):\n",
    "    \"\"\"\n",
    "    Train all 3 models\n",
    "    :param image: Whether or not to train the image model on this run\n",
    "    :param audio: Whether or not to train the audio model on this run\n",
    "    :param text: Whether or not to train the text model on this run\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    #To run natively do not use multiprocessing\n",
    "    if image:\n",
    "\n",
    "        # Parameters\n",
    "        params = {'dim': (10, 224, 224),\n",
    "                  'batch_size': 8,\n",
    "                  'n_channels': 3,\n",
    "                  'shuffle': True}\n",
    "\n",
    "        # Load labels set\n",
    "        with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "            training_labels = pickle.load(file)\n",
    "        with open('../data/image_data/pickle_files/y_5d_validation_all.pkl', 'rb') as file:\n",
    "            test_labels = pickle.load(file)\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(partition='training',\n",
    "                                           list_IDs=range(6000),\n",
    "                                           labels=training_labels, **params)\n",
    "        validation_generator = DataGenerator(partition='validation',\n",
    "                                             list_IDs=range(2000),\n",
    "                                             labels=test_labels, **params)\n",
    "        \n",
    "\n",
    "        # Create model\n",
    "        model = image_lrcn()\n",
    "        #model.load_weights('../output/image_model_vgg.h5')\n",
    "\n",
    "        # Train model on data set\n",
    "        t0 = np.load('../data/image_data/npy_files/test_data/0.npy')\n",
    "        pred0 = model.predict(t0)\n",
    "        print(pred0)\n",
    "        \n",
    "        t1 = np.load('../data/image_data/npy_files/test_data/1.npy')\n",
    "        pred1 = model.predict(t1)\n",
    "        print(pred1)\n",
    "        \n",
    "        model.fit_generator(generator=training_generator,\n",
    "                            validation_data=validation_generator,\n",
    "                            use_multiprocessing=False,\n",
    "                            workers=1,\n",
    "                            epochs=1)\n",
    "        t0 = np.load('../data/image_data/npy_files/test_data/0.npy')\n",
    "        pred0 = model.predict(t0)\n",
    "        print(pred0)\n",
    "        \n",
    "        t1 = np.load('../data/image_data/npy_files/test_data/1.npy')\n",
    "        pred1 = model.predict(t1)\n",
    "        print(pred1)\n",
    "\n",
    "        model.save_weights('../output/image_model_vgg_all.h5')\n",
    "\n",
    "    if audio:\n",
    "\n",
    "        # Read in audio data\n",
    "        training_set = pd.read_csv('../data/audio_data/pickle_files/training_df.csv')\n",
    "        test_set = pd.read_csv('../data/audio_data/pickle_files/validation_df.csv')\n",
    "\n",
    "        # Concat data sets in order to use all data for CV\n",
    "        all_data = pd.concat((training_set, test_set), axis=0)\n",
    "        X_all = all_data.drop(['interview_score', 'extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness', 'video_id'], axis=1)\n",
    "        y_all = all_data['interview_score', 'extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness']\n",
    "\n",
    "        logging.info('Start training audio model')\n",
    "\n",
    "        # Create model and fit to data\n",
    "        audio_model = models.audio_rand_forest()\n",
    "        audio_model.fit(X_all, y_all)\n",
    "\n",
    "        logging.info(audio_model.best_params_)\n",
    "        logging.info('Train score with best estimator: {}'.format(max(audio_model.cv_results_['mean_train_score'])))\n",
    "        logging.info('Validation score with best estimator: {}'.format(max(audio_model.cv_results_['mean_test_score'])))\n",
    "\n",
    "        # Save to disk\n",
    "        with open('../output/audio_model.pkl', 'wb') as fid:\n",
    "            pickle.dump(audio_model, fid)\n",
    "\n",
    "    if text:\n",
    "\n",
    "        # Load in word embeddings\n",
    "        embedding_matrix, word_to_index = resources.create_embedding_matrix()\n",
    "        print(len(word_to_index))\n",
    "\n",
    "        # Load text data\n",
    "        with open('../data/text_data/pickle_files/X_training.pkl', 'rb') as file:\n",
    "            X_train = pickle.load(file)\n",
    "        with open('../data/text_data/pickle_files/y_training.pkl', 'rb') as file:\n",
    "            y_train = pickle.load(file)\n",
    "        with open('../data/text_data/pickle_files/X_validation.pkl', 'rb') as file:\n",
    "            X_test = pickle.load(file)\n",
    "        with open('../data/text_data/pickle_files/y_validation.pkl', 'rb') as file:\n",
    "            y_test = pickle.load(file)\n",
    "        \n",
    "        print(X_test)\n",
    "\n",
    "        # Create model objec and fit\n",
    "        text_model = text_lstm_model(embedding_matrix=embedding_matrix)\n",
    "        filename = '../output/text_model_2.h5'\n",
    "        checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        text_model.fit(X_train, y_train,\n",
    "                       batch_size=16, epochs=55,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       callbacks=[checkpoint],\n",
    "                       shuffle=True)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "\n",
    "    print('Begin Ensemble model building, loading models')\n",
    "\n",
    "    # Load models\n",
    "    image_model = image_lrcn(0)\n",
    "    image_model.load_weights('../output/image_model_vgg_all.h5')\n",
    "    audio_model = pickle.load(open('../output/audio_model.pkl', 'rb'))\n",
    "    text_model = load_model('../output/text_model.h5')\n",
    "\n",
    "    # Load labels set\n",
    "    with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "        training_labels = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/y_5d_test_all.pkl', 'rb') as file:\n",
    "        test_labels = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/y_5d_validaion_all.pkl', 'rb') as file:\n",
    "        validation_labels = pickle.load(file)\n",
    "    # Load generators\n",
    "    training_generator = DataGenerator(partition='training', list_IDs=range(6000),\n",
    "                                       labels=training_labels, batch_size=8,\n",
    "                                       n_channels=3, dim=(10, 224, 224),\n",
    "                                       shuffle=False)\n",
    "    validation_generator = DataGenerator(partition='test', list_IDs=range(2000),\n",
    "                                         labels=test_labels, batch_size=8,\n",
    "                                         n_channels=3, dim=(10, 224, 224),\n",
    "                                         shuffle=False)\n",
    "    holdout_generator = DataGenerator(partition='validation', list_IDs=range(2000),\n",
    "                                      labels=validation_labels, batch_size=8,\n",
    "                                      n_channels=3, dim=(10, 224, 224),\n",
    "                                      shuffle=False)\n",
    "\n",
    "    print('Load data files')\n",
    "\n",
    "    # Load image data\n",
    "    with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "        y_img_train = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/y_5d_test_all.pkl', 'rb') as file:\n",
    "        y_img_test = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/y_5d_validation_all.pkl', 'rb') as file:\n",
    "        y_img_val = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/vid_ids_5d_training.pkl', 'rb') as file:\n",
    "        id_img_train = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/vid_ids_5d_test.pkl', 'rb') as file:\n",
    "        id_img_test = pickle.load(file)\n",
    "    with open('../data/image_data/pickle_files/vid_ids_5d_validation.pkl', 'rb') as file:\n",
    "        id_img_val = pickle.load(file)\n",
    "\n",
    "    # Load audio data\n",
    "    aud_train = pd.read_csv('../data/audio_data/pickle_files/training_df.csv')\n",
    "    aud_test = pd.read_csv('../data/audio_data/pickle_files/test_df.csv')\n",
    "    aud_val = pd.read_csv('../data/audio_data/pickle_files/validation_df.csv')\n",
    "    X_aud_train = aud_train.drop(['interview_score', 'video_id'], axis=1)\n",
    "    id_aud_train = aud_train['video_id']\n",
    "    X_aud_test = aud_test.drop(['interview_score', 'video_id'], axis=1)\n",
    "    id_aud_test = aud_test['video_id']\n",
    "    X_aud_val = aud_val.drop(['interview_score', 'video_id'], axis=1)\n",
    "    id_aud_val = aud_val['video_id']\n",
    "\n",
    "    # Load text data\n",
    "    with open('../data/text_data/pickle_files/X_training.pkl', 'rb') as file:\n",
    "        X_text_train = pickle.load(file)\n",
    "    with open('../data/text_data/pickle_files/X_test.pkl', 'rb') as file:\n",
    "        X_text_test = pickle.load(file)\n",
    "    with open('../data/text_data/pickle_files/X_validation.pkl', 'rb') as file:\n",
    "        X_text_val = pickle.load(file)\n",
    "    with open('../data/text_data/pickle_files/vid_ids_training.pkl', 'rb') as file:\n",
    "        id_text_train = pickle.load(file)\n",
    "    with open('../data/text_data/pickle_files/vid_ids_test.pkl', 'rb') as file:\n",
    "        id_text_test = pickle.load(file)\n",
    "    with open('../data/text_data/pickle_files/vid_ids_validation.pkl', 'rb') as file:\n",
    "        id_text_val = pickle.load(file)\n",
    "\n",
    "    print('Getting predictions for all 3 models')\n",
    "\n",
    "    # Get predictions\n",
    "    img_train_df = pd.DataFrame({'img_preds': [i[0] for i in image_model.predict_generator(training_generator)],\n",
    "                                 'video_ids': id_img_train,\n",
    "                                 'interview_score': y_img_train})\n",
    "    img_test_df = pd.DataFrame({'img_preds': [i[0] for i in image_model.predict_generator(validation_generator)],\n",
    "                                'video_ids': id_img_test,\n",
    "                                'interview_score':y_img_test})\n",
    "    img_val_df = pd.DataFrame({'img_preds': [i[0] for i in image_model.predict_generator(holdout_generator)],\n",
    "                               'video_ids': id_img_val,\n",
    "                               'interview_score': y_img_val})\n",
    "    aud_train_df = pd.DataFrame({'aud_preds': audio_model.predict(X_aud_train),\n",
    "                                 'video_ids': id_aud_train})\n",
    "    aud_test_df = pd.DataFrame({'aud_preds': audio_model.predict(X_aud_test),\n",
    "                                'video_ids': id_aud_test})\n",
    "    aud_val_df = pd.DataFrame({'aud_preds': audio_model.predict(X_aud_val),\n",
    "                               'video_ids': id_aud_val})\n",
    "    text_train_df = pd.DataFrame({'text_preds': [i[0] for i in text_model.predict(X_text_train)],\n",
    "                                  'video_ids': id_text_train})\n",
    "    text_test_df = pd.DataFrame({'text_preds': [i[0] for i in text_model.predict(X_text_test)],\n",
    "                                 'video_ids': id_text_test})\n",
    "    text_val_df = pd.DataFrame({'text_preds': [i[0] for i in text_model.predict(X_text_val)],\n",
    "                                'video_ids': id_text_val})\n",
    "\n",
    "    print('Merge predictions together into single data frame')\n",
    "\n",
    "    # Merge predictions\n",
    "    train_preds = img_train_df.merge(aud_train_df, on='video_ids')\n",
    "    train_preds = train_preds.merge(text_train_df, on='video_ids')\n",
    "    test_preds = img_test_df.merge(aud_test_df, on='video_ids')\n",
    "    test_preds = test_preds.merge(text_test_df, on='video_ids')\n",
    "    val_preds = img_val_df.merge(aud_val_df, on='video_ids')\n",
    "    val_preds = val_preds.merge(text_val_df, on='video_ids')\n",
    "\n",
    "    # Score models\n",
    "    img_train_score = np.sqrt(mean_squared_error(train_preds['interview_score'], train_preds['img_preds']))\n",
    "    img_test_score = np.sqrt(mean_squared_error(test_preds['interview_score'], test_preds['img_preds']))\n",
    "    img_val_score = np.sqrt(mean_squared_error(val_preds['interview_score'], val_preds['img_preds']))\n",
    "    aud_train_score = np.sqrt(mean_squared_error(train_preds['interview_score'], train_preds['aud_preds']))\n",
    "    aud_test_score = np.sqrt(mean_squared_error(test_preds['interview_score'], test_preds['aud_preds']))\n",
    "    aud_val_score = np.sqrt(mean_squared_error(val_preds['interview_score'], val_preds['aud_preds']))\n",
    "    text_train_score = np.sqrt(mean_squared_error(train_preds['interview_score'], train_preds['text_preds']))\n",
    "    text_test_score = np.sqrt(mean_squared_error(test_preds['interview_score'], test_preds['text_preds']))\n",
    "    text_val_score = np.sqrt(mean_squared_error(val_preds['interview_score'], val_preds['text_preds']))\n",
    "\n",
    "    # Print scores to screen\n",
    "    print('Image score on the training set: {}'.format(img_train_score))\n",
    "    print('Image score on the test set: {}'.format(img_test_score))\n",
    "    print('Image score on the val set: {}'.format(img_val_score))\n",
    "    print('Audio score on the training set: {}'.format(aud_train_score))\n",
    "    print('Audio score on the test set: {}'.format(aud_test_score))\n",
    "    print('Audio score on the val set: {}'.format(aud_val_score))\n",
    "    print('Text score on the training set: {}'.format(text_train_score))\n",
    "    print('Text score on the test set: {}'.format(text_test_score))\n",
    "    print('Text score on the val set: {}'.format(text_val_score))\n",
    "\n",
    "    # Split target variable and features\n",
    "    X_train = train_preds[['img_preds', 'aud_preds', 'text_preds']]\n",
    "    y_train = train_preds[['interview_score']]\n",
    "    X_test = test_preds[['img_preds', 'aud_preds', 'text_preds']]\n",
    "    y_test = test_preds[['interview_score']]\n",
    "    X_val = val_preds[['img_preds', 'aud_preds', 'text_preds']]\n",
    "    y_val = val_preds[['interview_score']]\n",
    "\n",
    "    print('Build OLS model to combine model outputs')\n",
    "\n",
    "    # Build OLS model\n",
    "    ols_model = LinearRegression()\n",
    "    ols_model.fit(X_train, y_train)\n",
    "    \n",
    "    pred_test = ols_model.predict(X_test)\n",
    "    #print(len(pred_test))\n",
    "\n",
    "    # Score model\n",
    "    train_score = np.sqrt(mean_squared_error(y_train, ols_model.predict(X_train)))\n",
    "    test_score = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "    val_score = np.sqrt(mean_squared_error(y_val, ols_model.predict(X_val)))\n",
    "    \n",
    "    #test value dataframe\n",
    "    '''test_df = pd.DataFrame(columns=['id','actual','pred'])\n",
    "    test_df['id'] = train_preds['video_ids']\n",
    "    test_df['actual'] = y_test\n",
    "    test_df['pred'] = pred_test\n",
    "    \n",
    "    test_df.to_csv('../output/prediction.csv', index = False)'''\n",
    "\n",
    "    print('OLS Score on training set: {}'.format(train_score))\n",
    "    print('OLS Score on test set: {}'.format(test_score))\n",
    "    print('OLS Score on val set: {}'.format(val_score))\n",
    "\n",
    "    # Save model\n",
    "    with open('../output/ensemble_model.pkl', 'wb') as fid:\n",
    "        pickle.dump(ols_model, fid)\n",
    "\n",
    "    logging.info('Ensemble model saved')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting targets from annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/meta_data/annotation_validation.pkl','rb') as f:\n",
    "        label_file = pickle.load(f, encoding='latin1')\n",
    "        #Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file['openness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data\n",
    "for partition in ['training', 'validation','test']:\n",
    "    with open('../data/meta_data/annotation_{}.pkl'.format(partition), 'rb') as f:\n",
    "        label_file = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # Get all IDs for videos for the training set\n",
    "    vid_ids = os.listdir('../data/image_data/{}_data'.format(partition))\n",
    "    file_ids = [i + '.mp4' for i in vid_ids]\n",
    "\n",
    "    \n",
    "    y_interview = [label_file['interview'][i + '.mp4'] for i in vid_ids]\n",
    "    y_extraversion = [label_file['extraversion'][i + '.mp4'] for i in vid_ids]\n",
    "    y_agreeableness = [label_file['agreeableness'][i + '.mp4'] for i in vid_ids]\n",
    "    y_conscientiousness = [label_file['conscientiousness'][i + '.mp4'] for i in vid_ids]\n",
    "    y_neuroticism = [label_file['neuroticism'][i + '.mp4'] for i in vid_ids]\n",
    "    y_openness = [label_file['openness'][i + '.mp4'] for i in vid_ids]\n",
    "    \n",
    "    y = np.zeros((len(y_interview), 6))\n",
    "    for i in range(len(y_interview)):\n",
    "        y[i][0] = y_interview[i]\n",
    "        y[i][1] = y_extraversion[i]\n",
    "        y[i][2] = y_agreeableness[i]\n",
    "        y[i][3] = y_conscientiousness[i]\n",
    "        y[i][4] = y_neuroticism[i]\n",
    "        y[i][5] = y_openness[i]\n",
    "    \n",
    "    with open('../data/image_data/pickle_files/y_5d_{}_all.pkl'.format(partition), 'wb') as output:\n",
    "        pickle.dump(y, output, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio data\n",
    "for partition in ['training', 'validation','test']:\n",
    "    with open('../data/meta_data/annotation_{}.pkl'.format(partition), 'rb') as f:\n",
    "            label_file = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    audio_files = os.listdir('../data/audio_data/{}_data'.format(partition))\n",
    "    audio_files = [i.split('.wav')[0] for i in audio_files]\n",
    "    id_array = [i + '.mp4' for i in audio_files]\n",
    "\n",
    "\n",
    "    score_interview = [label_file['interview'][i + '.mp4'] for i in audio_files]\n",
    "    score_extraversion = [label_file['extraversion'][i + '.mp4'] for i in audio_files]\n",
    "    score_agreeableness = [label_file['agreeableness'][i + '.mp4'] for i in audio_files]\n",
    "    score_conscientiousness = [label_file['conscientiousness'][i + '.mp4'] for i in audio_files]\n",
    "    score_neuroticism = [label_file['neuroticism'][i + '.mp4'] for i in audio_files]\n",
    "    score_openness = [label_file['openness'][i + '.mp4'] for i in audio_files]\n",
    "    \n",
    "    score_interview = np.array(score_interview)\n",
    "    score_extraversion = np.array(score_extraversion)\n",
    "    score_agreeableness = np.array(score_agreeableness)\n",
    "    score_conscientiousness = np.array(score_conscientiousness)\n",
    "    score_neuroticism = np.array(score_neuroticism)\n",
    "    score_openness = np.array(score_openness)\n",
    "    \n",
    "    audio_df = pd.DataFrame(audio_matrix, columns=cols)\n",
    "    audio_df['interview_score'] = score_interview\n",
    "    audio_df['extraversion'] = score_extraversion\n",
    "    audio_df['agreeableness'] = score_agreeableness\n",
    "    audio_df['conscientiousness'] = score_conscientiousness\n",
    "    audio_df['neuroticism'] = score_neuroticism\n",
    "    audio_df['openness'] = score_openness\n",
    "    \n",
    "    audio_df['video_id'] = id_array\n",
    "\n",
    "    #print(id_array[0:20])\n",
    "\n",
    "    audio_df.to_csv('../data/audio_data/pickle_files/{}_df.csv'.format(partition, partition), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
