{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "training_data_path = \"../data/image_data/training_data/\"    # define UCF-101 RGB data path\n",
    "validation_data_path = \"../data/image_data/validation_data/\"\n",
    "test_data_path = \"../data/image_data/test_data/\"\n",
    "save_model_path = \"./ResNetCRNN_ckpt/\"\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.25    # dropout probability\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 6           # number of target category\n",
    "epochs = 50       # training epochs\n",
    "batch_size = 128\n",
    "learning_rate = 7e-6\n",
    "l_decay = 5e-4\n",
    "log_interval = 10  # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 20, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        \n",
    "        criterion = nn.L1Loss(reduction = 'sum')\n",
    "        loss = criterion(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        #y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        #step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        #scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            \n",
    "            criterion = nn.L1Loss(reduction = 'sum')\n",
    "            loss = criterion(output, y)\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            #all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    #all_y = torch.stack(all_y, dim=0)\n",
    "    #all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    #test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.8f}, \\n'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRNN_final_prediction(model, device, loader):\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(tqdm(loader)):\n",
    "            # distribute data to device\n",
    "            X = X.to(device)\n",
    "            output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n",
    "            all_y_pred.append(output)\n",
    "\n",
    "    return all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "params2 = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "    train_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_training.pkl', 'rb') as file:\n",
    "    train_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_validation_all.pkl', 'rb') as file:\n",
    "    val_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_validation.pkl', 'rb') as file:\n",
    "    val_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_test_all.pkl', 'rb') as file:\n",
    "    test_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_test.pkl', 'rb') as file:\n",
    "    test_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = Dataset_CRNN(training_data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN(validation_data_path, val_list, val_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_CRNN(test_data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_net = Final_Net(drop_p = dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPU!\n"
     ]
    }
   ],
   "source": [
    "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "# Combine all EncoderCNN + DecoderRNN parameters\n",
    "crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "              list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "              list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": cnn_encoder.resnet.parameters(), \"lr\": 1e-6},\n",
    "                {\"params\": crnn_params}\n",
    "            ],\n",
    "            lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 54.970566\n",
      "Train Epoch: 1 [2560/6000 (43%)]\tLoss: 57.148220\n",
      "Train Epoch: 1 [3840/6000 (64%)]\tLoss: 51.365261\n",
      "Train Epoch: 1 [5120/6000 (85%)]\tLoss: 51.518131\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58277439, \n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [1280/6000 (21%)]\tLoss: 53.027069\n",
      "Train Epoch: 2 [2560/6000 (43%)]\tLoss: 52.080971\n",
      "Train Epoch: 2 [3840/6000 (64%)]\tLoss: 51.864014\n",
      "Train Epoch: 2 [5120/6000 (85%)]\tLoss: 54.606243\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58588967, \n",
      "\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [1280/6000 (21%)]\tLoss: 49.806267\n",
      "Train Epoch: 3 [2560/6000 (43%)]\tLoss: 50.987175\n",
      "Train Epoch: 3 [3840/6000 (64%)]\tLoss: 52.715179\n",
      "Train Epoch: 3 [5120/6000 (85%)]\tLoss: 51.381561\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58813618, \n",
      "\n",
      "Epoch 3 model saved!\n",
      "Train Epoch: 4 [1280/6000 (21%)]\tLoss: 55.628841\n",
      "Train Epoch: 4 [2560/6000 (43%)]\tLoss: 60.013023\n",
      "Train Epoch: 4 [3840/6000 (64%)]\tLoss: 51.213821\n",
      "Train Epoch: 4 [5120/6000 (85%)]\tLoss: 51.452766\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58603118, \n",
      "\n",
      "Epoch 4 model saved!\n",
      "Train Epoch: 5 [1280/6000 (21%)]\tLoss: 54.716648\n",
      "Train Epoch: 5 [2560/6000 (43%)]\tLoss: 50.508038\n",
      "Train Epoch: 5 [3840/6000 (64%)]\tLoss: 48.590199\n",
      "Train Epoch: 5 [5120/6000 (85%)]\tLoss: 48.861172\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58902896, \n",
      "\n",
      "Epoch 5 model saved!\n",
      "Train Epoch: 6 [1280/6000 (21%)]\tLoss: 51.909061\n",
      "Train Epoch: 6 [2560/6000 (43%)]\tLoss: 51.149998\n",
      "Train Epoch: 6 [3840/6000 (64%)]\tLoss: 48.710487\n",
      "Train Epoch: 6 [5120/6000 (85%)]\tLoss: 49.638680\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58856144, \n",
      "\n",
      "Epoch 6 model saved!\n",
      "Train Epoch: 7 [1280/6000 (21%)]\tLoss: 52.362373\n",
      "Train Epoch: 7 [2560/6000 (43%)]\tLoss: 49.184837\n",
      "Train Epoch: 7 [3840/6000 (64%)]\tLoss: 51.850105\n",
      "Train Epoch: 7 [5120/6000 (85%)]\tLoss: 50.347820\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58906646, \n",
      "\n",
      "Epoch 7 model saved!\n",
      "Train Epoch: 8 [1280/6000 (21%)]\tLoss: 47.979225\n",
      "Train Epoch: 8 [2560/6000 (43%)]\tLoss: 48.666218\n",
      "Train Epoch: 8 [3840/6000 (64%)]\tLoss: 55.880352\n",
      "Train Epoch: 8 [5120/6000 (85%)]\tLoss: 51.485111\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.59037374, \n",
      "\n",
      "Epoch 8 model saved!\n",
      "Train Epoch: 9 [1280/6000 (21%)]\tLoss: 51.318035\n",
      "Train Epoch: 9 [2560/6000 (43%)]\tLoss: 52.005814\n",
      "Train Epoch: 9 [3840/6000 (64%)]\tLoss: 49.361012\n",
      "Train Epoch: 9 [5120/6000 (85%)]\tLoss: 53.529480\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58463666, \n",
      "\n",
      "Epoch 9 model saved!\n",
      "Train Epoch: 10 [1280/6000 (21%)]\tLoss: 51.489304\n",
      "Train Epoch: 10 [2560/6000 (43%)]\tLoss: 44.265373\n",
      "Train Epoch: 10 [3840/6000 (64%)]\tLoss: 49.374123\n",
      "Train Epoch: 10 [5120/6000 (85%)]\tLoss: 48.197891\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58438926, \n",
      "\n",
      "Epoch 10 model saved!\n",
      "Train Epoch: 11 [1280/6000 (21%)]\tLoss: 48.584019\n",
      "Train Epoch: 11 [2560/6000 (43%)]\tLoss: 49.331177\n",
      "Train Epoch: 11 [3840/6000 (64%)]\tLoss: 46.919952\n",
      "Train Epoch: 11 [5120/6000 (85%)]\tLoss: 48.972580\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58410071, \n",
      "\n",
      "Epoch 11 model saved!\n",
      "Train Epoch: 12 [1280/6000 (21%)]\tLoss: 49.652924\n",
      "Train Epoch: 12 [2560/6000 (43%)]\tLoss: 46.656151\n",
      "Train Epoch: 12 [3840/6000 (64%)]\tLoss: 47.020866\n",
      "Train Epoch: 12 [5120/6000 (85%)]\tLoss: 48.909645\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58562171, \n",
      "\n",
      "Epoch 12 model saved!\n",
      "Train Epoch: 13 [1280/6000 (21%)]\tLoss: 51.839474\n",
      "Train Epoch: 13 [2560/6000 (43%)]\tLoss: 49.032787\n",
      "Train Epoch: 13 [3840/6000 (64%)]\tLoss: 46.796982\n",
      "Train Epoch: 13 [5120/6000 (85%)]\tLoss: 49.365776\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58120248, \n",
      "\n",
      "Epoch 13 model saved!\n",
      "Train Epoch: 14 [1280/6000 (21%)]\tLoss: 47.155083\n",
      "Train Epoch: 14 [2560/6000 (43%)]\tLoss: 47.404335\n",
      "Train Epoch: 14 [3840/6000 (64%)]\tLoss: 54.436958\n",
      "Train Epoch: 14 [5120/6000 (85%)]\tLoss: 50.720261\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58000698, \n",
      "\n",
      "Epoch 14 model saved!\n",
      "Train Epoch: 15 [1280/6000 (21%)]\tLoss: 47.363331\n",
      "Train Epoch: 15 [2560/6000 (43%)]\tLoss: 45.799206\n",
      "Train Epoch: 15 [3840/6000 (64%)]\tLoss: 49.357941\n",
      "Train Epoch: 15 [5120/6000 (85%)]\tLoss: 50.827835\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58328296, \n",
      "\n",
      "Epoch 15 model saved!\n",
      "Train Epoch: 16 [1280/6000 (21%)]\tLoss: 46.704082\n",
      "Train Epoch: 16 [2560/6000 (43%)]\tLoss: 46.808685\n",
      "Train Epoch: 16 [3840/6000 (64%)]\tLoss: 47.742744\n",
      "Train Epoch: 16 [5120/6000 (85%)]\tLoss: 48.081207\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.57926606, \n",
      "\n",
      "Epoch 16 model saved!\n",
      "Train Epoch: 17 [1280/6000 (21%)]\tLoss: 46.340984\n",
      "Train Epoch: 17 [2560/6000 (43%)]\tLoss: 47.141254\n",
      "Train Epoch: 17 [3840/6000 (64%)]\tLoss: 46.166618\n",
      "Train Epoch: 17 [5120/6000 (85%)]\tLoss: 46.198948\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.58482585, \n",
      "\n",
      "Epoch 17 model saved!\n",
      "Train Epoch: 18 [1280/6000 (21%)]\tLoss: 45.786076\n",
      "Train Epoch: 18 [2560/6000 (43%)]\tLoss: 45.680115\n",
      "Train Epoch: 18 [3840/6000 (64%)]\tLoss: 44.762230\n",
      "Train Epoch: 18 [5120/6000 (85%)]\tLoss: 51.330570\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.57938779, \n",
      "\n",
      "Epoch 18 model saved!\n",
      "Train Epoch: 19 [1280/6000 (21%)]\tLoss: 44.983147\n",
      "Train Epoch: 19 [2560/6000 (43%)]\tLoss: 47.476402\n",
      "Train Epoch: 19 [3840/6000 (64%)]\tLoss: 49.321232\n",
      "Train Epoch: 19 [5120/6000 (85%)]\tLoss: 45.706615\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    if __name__ == '__main__':\n",
    "        train_losses, train_scores = train(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "        epoch_test_loss = validation([cnn_encoder, rnn_decoder], device, optimizer, test_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    np.save('./CRNN_epoch_training_losses.npy', A)\n",
    "    np.save('./CRNN_epoch_test_loss.npy', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_CRNN(test_data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'cnn_encoder_epoch10.pth')))\n",
    "rnn_decoder.load_state_dict(torch.load(os.path.join(save_model_path, 'rnn_decoder_epoch10.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:14<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "all_y_pred = CRNN_final_prediction([cnn_encoder, rnn_decoder], device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((2000,6))\n",
    "k = 0\n",
    "for i in range(len(all_y_pred)):\n",
    "    for j in range(len(all_y_pred[i])):\n",
    "        batch_pred = all_y_pred[i].cpu()\n",
    "        preds[k] = batch_pred[j]\n",
    "        k +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52528113 0.47713935 0.56478775 0.54865766 0.53525519 0.55902994]\n",
      " [0.40809232 0.38457096 0.48803568 0.42725697 0.42693549 0.47648066]\n",
      " [0.50756526 0.46386525 0.56816781 0.52261978 0.5219897  0.55389333]\n",
      " ...\n",
      " [0.59813333 0.59519273 0.59381807 0.60859209 0.62060815 0.68117052]\n",
      " [0.5947057  0.51854926 0.62815386 0.62048274 0.59751427 0.59946895]\n",
      " [0.36433494 0.34954178 0.45114994 0.38957319 0.3871204  0.45422906]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41121495 0.42990654 0.50549451 0.45631068 0.53125    0.46666667]\n",
      " [0.3271028  0.27102804 0.47252747 0.31067961 0.33333333 0.43333333]\n",
      " [0.4953271  0.35514019 0.52747253 0.67961165 0.39583333 0.45555556]\n",
      " ...\n",
      " [0.6728972  0.53271028 0.54945055 0.73786408 0.63541667 0.81111111]\n",
      " [0.57009346 0.40186916 0.65934066 0.55339806 0.53125    0.54444444]\n",
      " [0.41121495 0.28037383 0.50549451 0.48543689 0.35416667 0.37777778]]\n"
     ]
    }
   ],
   "source": [
    "print(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Metrics IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90598967 0.89957138 0.90615072 0.90091895 0.89888316 0.90227199]\n",
      "0.9022976455721116\n"
     ]
    }
   ],
   "source": [
    "acc = np.zeros(6)\n",
    "for i in range(6):\n",
    "    ind = i\n",
    "    diff = abs(preds[:,ind] - test_label[:, ind])\n",
    "\n",
    "    acc[i] = 1-(np.sum(diff))/2000\n",
    "\n",
    "print(acc)\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5073738317757008\n",
      "0.9996010760463486\n"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "\n",
    "diff = abs(preds[:,ind] - test_label[:, ind])\n",
    "avg = np.mean(test_label[:, ind])\n",
    "print(avg)\n",
    "avg_diff = abs(test_label[:, ind] - avg)\n",
    "\n",
    "acc = 1 - ((np.sum(diff))/np.sum(avg_diff))/2000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-93a65799236c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train loss (on epoch end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m#  test loss (on epoch end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "plt.title(\"model loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "training_data_path = \"../data/image_data/training_data/\"    # define UCF-101 RGB data path\n",
    "validation_data_path = \"../data/image_data/validation_data/\"\n",
    "test_data_path = \"../data/image_data/test_data/\"\n",
    "save_model_path = \"./ResNetCRNN_ckpt/\"\n",
    "\n",
    "# ResnetEncoder architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.25    # dropout probability\n",
    "\n",
    "# Not of use here\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 128\n",
    "\n",
    "# training parameters\n",
    "k = 6           # number of target category\n",
    "epochs = 50       # training epochs\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "l_decay = 5e-4\n",
    "log_interval = 10  # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    resnet_encoder = model\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        video_output = resnet_encoder(X)   # output has dim = (batch, number of classes)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(video_output, y))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        #y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        #step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        #scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    resnet_encoder = model\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            video_output = resnet_encoder(X)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = torch.sqrt(criterion(video_output, y))\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            #all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    #all_y = torch.stack(all_y, dim=0)\n",
    "    #all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    #test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, \\n'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(resnet_encoder.state_dict(), os.path.join(save_model_path, 'resnet_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_resnet_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRNN_final_prediction(model, device, loader):\n",
    "    resnet_encoder = model\n",
    "\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(tqdm(loader)):\n",
    "            # distribute data to device\n",
    "            X = X.to(device)\n",
    "            video_output = resnet_encoder(X)   # output has dim = (batch, number of classes)\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n",
    "            all_y_pred.append(video_output)\n",
    "\n",
    "    return all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "params2 = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                \n",
    "                               ])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "    train_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_training.pkl', 'rb') as file:\n",
    "    train_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_validation_all.pkl', 'rb') as file:\n",
    "    val_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_validation.pkl', 'rb') as file:\n",
    "    val_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_test_all.pkl', 'rb') as file:\n",
    "    test_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_test.pkl', 'rb') as file:\n",
    "    test_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = Dataset_Resnet(training_data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_Resnet(validation_data_path, val_list, val_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_Resnet(test_data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_encoder = ResnetEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPU!\n"
     ]
    }
   ],
   "source": [
    "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "# Combine all EncoderCNN + DecoderRNN parameters\n",
    "resnet_params = list(resnet_encoder.fc1.parameters()) + list(resnet_encoder.bn1.parameters()) + \\\n",
    "              list(resnet_encoder.fc2.parameters()) + list(resnet_encoder.bn2.parameters()) + \\\n",
    "              list(resnet_encoder.fc3.parameters()) + list(resnet_encoder.fc4.parameters()) + list(resnet_encoder.fc5.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(resnet_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [320/6000 (5%)]\tLoss: 0.129187\n",
      "Train Epoch: 1 [640/6000 (11%)]\tLoss: 0.138105\n",
      "Train Epoch: 1 [960/6000 (16%)]\tLoss: 0.133177\n",
      "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 0.131702\n",
      "Train Epoch: 1 [1600/6000 (27%)]\tLoss: 0.129968\n",
      "Train Epoch: 1 [1920/6000 (32%)]\tLoss: 0.103351\n",
      "Train Epoch: 1 [2240/6000 (37%)]\tLoss: 0.141724\n",
      "Train Epoch: 1 [2560/6000 (43%)]\tLoss: 0.118666\n",
      "Train Epoch: 1 [2880/6000 (48%)]\tLoss: 0.110001\n",
      "Train Epoch: 1 [3200/6000 (53%)]\tLoss: 0.119945\n",
      "Train Epoch: 1 [3520/6000 (59%)]\tLoss: 0.143460\n",
      "Train Epoch: 1 [3840/6000 (64%)]\tLoss: 0.133383\n",
      "Train Epoch: 1 [4160/6000 (69%)]\tLoss: 0.129440\n",
      "Train Epoch: 1 [4480/6000 (74%)]\tLoss: 0.113232\n",
      "Train Epoch: 1 [4800/6000 (80%)]\tLoss: 0.116389\n",
      "Train Epoch: 1 [5120/6000 (85%)]\tLoss: 0.116457\n",
      "Train Epoch: 1 [5440/6000 (90%)]\tLoss: 0.124644\n",
      "Train Epoch: 1 [5760/6000 (96%)]\tLoss: 0.118416\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0041, \n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [320/6000 (5%)]\tLoss: 0.129512\n",
      "Train Epoch: 2 [640/6000 (11%)]\tLoss: 0.129001\n",
      "Train Epoch: 2 [960/6000 (16%)]\tLoss: 0.123941\n",
      "Train Epoch: 2 [1280/6000 (21%)]\tLoss: 0.099455\n",
      "Train Epoch: 2 [1600/6000 (27%)]\tLoss: 0.126496\n",
      "Train Epoch: 2 [1920/6000 (32%)]\tLoss: 0.133088\n",
      "Train Epoch: 2 [2240/6000 (37%)]\tLoss: 0.115630\n",
      "Train Epoch: 2 [2560/6000 (43%)]\tLoss: 0.113495\n",
      "Train Epoch: 2 [2880/6000 (48%)]\tLoss: 0.138716\n",
      "Train Epoch: 2 [3200/6000 (53%)]\tLoss: 0.120084\n",
      "Train Epoch: 2 [3520/6000 (59%)]\tLoss: 0.124021\n",
      "Train Epoch: 2 [3840/6000 (64%)]\tLoss: 0.118386\n",
      "Train Epoch: 2 [4160/6000 (69%)]\tLoss: 0.128003\n",
      "Train Epoch: 2 [4480/6000 (74%)]\tLoss: 0.134070\n",
      "Train Epoch: 2 [4800/6000 (80%)]\tLoss: 0.126096\n",
      "Train Epoch: 2 [5120/6000 (85%)]\tLoss: 0.118187\n",
      "Train Epoch: 2 [5440/6000 (90%)]\tLoss: 0.123452\n",
      "Train Epoch: 2 [5760/6000 (96%)]\tLoss: 0.120560\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [320/6000 (5%)]\tLoss: 0.118001\n",
      "Train Epoch: 3 [640/6000 (11%)]\tLoss: 0.139099\n",
      "Train Epoch: 3 [960/6000 (16%)]\tLoss: 0.125815\n",
      "Train Epoch: 3 [1280/6000 (21%)]\tLoss: 0.097599\n",
      "Train Epoch: 3 [1600/6000 (27%)]\tLoss: 0.123326\n",
      "Train Epoch: 3 [1920/6000 (32%)]\tLoss: 0.113368\n",
      "Train Epoch: 3 [2240/6000 (37%)]\tLoss: 0.141439\n",
      "Train Epoch: 3 [2560/6000 (43%)]\tLoss: 0.116833\n",
      "Train Epoch: 3 [2880/6000 (48%)]\tLoss: 0.115445\n",
      "Train Epoch: 3 [3200/6000 (53%)]\tLoss: 0.117193\n",
      "Train Epoch: 3 [3520/6000 (59%)]\tLoss: 0.111453\n",
      "Train Epoch: 3 [3840/6000 (64%)]\tLoss: 0.131467\n",
      "Train Epoch: 3 [4160/6000 (69%)]\tLoss: 0.120452\n",
      "Train Epoch: 3 [4480/6000 (74%)]\tLoss: 0.117826\n",
      "Train Epoch: 3 [4800/6000 (80%)]\tLoss: 0.137327\n",
      "Train Epoch: 3 [5120/6000 (85%)]\tLoss: 0.121819\n",
      "Train Epoch: 3 [5440/6000 (90%)]\tLoss: 0.115543\n",
      "Train Epoch: 3 [5760/6000 (96%)]\tLoss: 0.129789\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 3 model saved!\n",
      "Train Epoch: 4 [320/6000 (5%)]\tLoss: 0.122823\n",
      "Train Epoch: 4 [640/6000 (11%)]\tLoss: 0.119469\n",
      "Train Epoch: 4 [960/6000 (16%)]\tLoss: 0.103216\n",
      "Train Epoch: 4 [1280/6000 (21%)]\tLoss: 0.120117\n",
      "Train Epoch: 4 [1600/6000 (27%)]\tLoss: 0.105404\n",
      "Train Epoch: 4 [1920/6000 (32%)]\tLoss: 0.116598\n",
      "Train Epoch: 4 [2240/6000 (37%)]\tLoss: 0.118023\n",
      "Train Epoch: 4 [2560/6000 (43%)]\tLoss: 0.130954\n",
      "Train Epoch: 4 [2880/6000 (48%)]\tLoss: 0.109067\n",
      "Train Epoch: 4 [3200/6000 (53%)]\tLoss: 0.116659\n",
      "Train Epoch: 4 [3520/6000 (59%)]\tLoss: 0.121662\n",
      "Train Epoch: 4 [3840/6000 (64%)]\tLoss: 0.126691\n",
      "Train Epoch: 4 [4160/6000 (69%)]\tLoss: 0.108426\n",
      "Train Epoch: 4 [4480/6000 (74%)]\tLoss: 0.098754\n",
      "Train Epoch: 4 [4800/6000 (80%)]\tLoss: 0.116412\n",
      "Train Epoch: 4 [5120/6000 (85%)]\tLoss: 0.108028\n",
      "Train Epoch: 4 [5440/6000 (90%)]\tLoss: 0.128091\n",
      "Train Epoch: 4 [5760/6000 (96%)]\tLoss: 0.125145\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 4 model saved!\n",
      "Train Epoch: 5 [320/6000 (5%)]\tLoss: 0.110730\n",
      "Train Epoch: 5 [640/6000 (11%)]\tLoss: 0.102488\n",
      "Train Epoch: 5 [960/6000 (16%)]\tLoss: 0.110704\n",
      "Train Epoch: 5 [1280/6000 (21%)]\tLoss: 0.112919\n",
      "Train Epoch: 5 [1600/6000 (27%)]\tLoss: 0.144491\n",
      "Train Epoch: 5 [1920/6000 (32%)]\tLoss: 0.109015\n",
      "Train Epoch: 5 [2240/6000 (37%)]\tLoss: 0.124672\n",
      "Train Epoch: 5 [2560/6000 (43%)]\tLoss: 0.091225\n",
      "Train Epoch: 5 [2880/6000 (48%)]\tLoss: 0.145899\n",
      "Train Epoch: 5 [3200/6000 (53%)]\tLoss: 0.108155\n",
      "Train Epoch: 5 [3520/6000 (59%)]\tLoss: 0.139494\n",
      "Train Epoch: 5 [3840/6000 (64%)]\tLoss: 0.132135\n",
      "Train Epoch: 5 [4160/6000 (69%)]\tLoss: 0.121027\n",
      "Train Epoch: 5 [4480/6000 (74%)]\tLoss: 0.123060\n",
      "Train Epoch: 5 [4800/6000 (80%)]\tLoss: 0.105451\n",
      "Train Epoch: 5 [5120/6000 (85%)]\tLoss: 0.115627\n",
      "Train Epoch: 5 [5440/6000 (90%)]\tLoss: 0.118718\n",
      "Train Epoch: 5 [5760/6000 (96%)]\tLoss: 0.113660\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 5 model saved!\n",
      "Train Epoch: 6 [320/6000 (5%)]\tLoss: 0.099484\n",
      "Train Epoch: 6 [640/6000 (11%)]\tLoss: 0.121135\n",
      "Train Epoch: 6 [960/6000 (16%)]\tLoss: 0.109831\n",
      "Train Epoch: 6 [1280/6000 (21%)]\tLoss: 0.115174\n",
      "Train Epoch: 6 [1600/6000 (27%)]\tLoss: 0.093623\n",
      "Train Epoch: 6 [1920/6000 (32%)]\tLoss: 0.138809\n",
      "Train Epoch: 6 [2240/6000 (37%)]\tLoss: 0.123574\n",
      "Train Epoch: 6 [2560/6000 (43%)]\tLoss: 0.111255\n",
      "Train Epoch: 6 [2880/6000 (48%)]\tLoss: 0.119683\n",
      "Train Epoch: 6 [3200/6000 (53%)]\tLoss: 0.099347\n",
      "Train Epoch: 6 [3520/6000 (59%)]\tLoss: 0.131108\n",
      "Train Epoch: 6 [3840/6000 (64%)]\tLoss: 0.096742\n",
      "Train Epoch: 6 [4160/6000 (69%)]\tLoss: 0.096047\n",
      "Train Epoch: 6 [4480/6000 (74%)]\tLoss: 0.105710\n",
      "Train Epoch: 6 [4800/6000 (80%)]\tLoss: 0.103128\n",
      "Train Epoch: 6 [5120/6000 (85%)]\tLoss: 0.099015\n",
      "Train Epoch: 6 [5440/6000 (90%)]\tLoss: 0.107456\n",
      "Train Epoch: 6 [5760/6000 (96%)]\tLoss: 0.113894\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 6 model saved!\n",
      "Train Epoch: 7 [320/6000 (5%)]\tLoss: 0.125193\n",
      "Train Epoch: 7 [640/6000 (11%)]\tLoss: 0.103196\n",
      "Train Epoch: 7 [960/6000 (16%)]\tLoss: 0.119898\n",
      "Train Epoch: 7 [1280/6000 (21%)]\tLoss: 0.106515\n",
      "Train Epoch: 7 [1600/6000 (27%)]\tLoss: 0.118471\n",
      "Train Epoch: 7 [1920/6000 (32%)]\tLoss: 0.106672\n",
      "Train Epoch: 7 [2240/6000 (37%)]\tLoss: 0.118910\n",
      "Train Epoch: 7 [2560/6000 (43%)]\tLoss: 0.105841\n",
      "Train Epoch: 7 [2880/6000 (48%)]\tLoss: 0.112095\n",
      "Train Epoch: 7 [3200/6000 (53%)]\tLoss: 0.099078\n",
      "Train Epoch: 7 [3520/6000 (59%)]\tLoss: 0.112768\n",
      "Train Epoch: 7 [3840/6000 (64%)]\tLoss: 0.119559\n",
      "Train Epoch: 7 [4160/6000 (69%)]\tLoss: 0.123188\n",
      "Train Epoch: 7 [4480/6000 (74%)]\tLoss: 0.104631\n",
      "Train Epoch: 7 [4800/6000 (80%)]\tLoss: 0.103117\n",
      "Train Epoch: 7 [5120/6000 (85%)]\tLoss: 0.109995\n",
      "Train Epoch: 7 [5440/6000 (90%)]\tLoss: 0.114530\n",
      "Train Epoch: 7 [5760/6000 (96%)]\tLoss: 0.104558\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 7 model saved!\n",
      "Train Epoch: 8 [320/6000 (5%)]\tLoss: 0.140094\n",
      "Train Epoch: 8 [640/6000 (11%)]\tLoss: 0.113685\n",
      "Train Epoch: 8 [960/6000 (16%)]\tLoss: 0.107120\n",
      "Train Epoch: 8 [1280/6000 (21%)]\tLoss: 0.114925\n",
      "Train Epoch: 8 [1600/6000 (27%)]\tLoss: 0.108853\n",
      "Train Epoch: 8 [1920/6000 (32%)]\tLoss: 0.104088\n",
      "Train Epoch: 8 [2240/6000 (37%)]\tLoss: 0.110854\n",
      "Train Epoch: 8 [2560/6000 (43%)]\tLoss: 0.104794\n",
      "Train Epoch: 8 [2880/6000 (48%)]\tLoss: 0.117873\n",
      "Train Epoch: 8 [3200/6000 (53%)]\tLoss: 0.091682\n",
      "Train Epoch: 8 [3520/6000 (59%)]\tLoss: 0.121132\n",
      "Train Epoch: 8 [3840/6000 (64%)]\tLoss: 0.103424\n",
      "Train Epoch: 8 [4160/6000 (69%)]\tLoss: 0.112644\n",
      "Train Epoch: 8 [4480/6000 (74%)]\tLoss: 0.103394\n",
      "Train Epoch: 8 [4800/6000 (80%)]\tLoss: 0.116779\n",
      "Train Epoch: 8 [5120/6000 (85%)]\tLoss: 0.116821\n",
      "Train Epoch: 8 [5440/6000 (90%)]\tLoss: 0.118082\n",
      "Train Epoch: 8 [5760/6000 (96%)]\tLoss: 0.109160\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 8 model saved!\n",
      "Train Epoch: 9 [320/6000 (5%)]\tLoss: 0.110528\n",
      "Train Epoch: 9 [640/6000 (11%)]\tLoss: 0.099887\n",
      "Train Epoch: 9 [960/6000 (16%)]\tLoss: 0.102125\n",
      "Train Epoch: 9 [1280/6000 (21%)]\tLoss: 0.110692\n",
      "Train Epoch: 9 [1600/6000 (27%)]\tLoss: 0.115642\n",
      "Train Epoch: 9 [1920/6000 (32%)]\tLoss: 0.095347\n",
      "Train Epoch: 9 [2240/6000 (37%)]\tLoss: 0.092585\n",
      "Train Epoch: 9 [2560/6000 (43%)]\tLoss: 0.095326\n",
      "Train Epoch: 9 [2880/6000 (48%)]\tLoss: 0.089258\n",
      "Train Epoch: 9 [3200/6000 (53%)]\tLoss: 0.115727\n",
      "Train Epoch: 9 [3520/6000 (59%)]\tLoss: 0.101740\n",
      "Train Epoch: 9 [3840/6000 (64%)]\tLoss: 0.107503\n",
      "Train Epoch: 9 [4160/6000 (69%)]\tLoss: 0.099426\n",
      "Train Epoch: 9 [4480/6000 (74%)]\tLoss: 0.105747\n",
      "Train Epoch: 9 [4800/6000 (80%)]\tLoss: 0.102331\n",
      "Train Epoch: 9 [5120/6000 (85%)]\tLoss: 0.102526\n",
      "Train Epoch: 9 [5440/6000 (90%)]\tLoss: 0.107982\n",
      "Train Epoch: 9 [5760/6000 (96%)]\tLoss: 0.106663\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 9 model saved!\n",
      "Train Epoch: 10 [320/6000 (5%)]\tLoss: 0.090331\n",
      "Train Epoch: 10 [640/6000 (11%)]\tLoss: 0.110280\n",
      "Train Epoch: 10 [960/6000 (16%)]\tLoss: 0.118816\n",
      "Train Epoch: 10 [1280/6000 (21%)]\tLoss: 0.107810\n",
      "Train Epoch: 10 [1600/6000 (27%)]\tLoss: 0.117848\n",
      "Train Epoch: 10 [1920/6000 (32%)]\tLoss: 0.115932\n",
      "Train Epoch: 10 [2240/6000 (37%)]\tLoss: 0.106213\n",
      "Train Epoch: 10 [2560/6000 (43%)]\tLoss: 0.099242\n",
      "Train Epoch: 10 [2880/6000 (48%)]\tLoss: 0.089668\n",
      "Train Epoch: 10 [3200/6000 (53%)]\tLoss: 0.099540\n",
      "Train Epoch: 10 [3520/6000 (59%)]\tLoss: 0.114521\n",
      "Train Epoch: 10 [3840/6000 (64%)]\tLoss: 0.110017\n",
      "Train Epoch: 10 [4160/6000 (69%)]\tLoss: 0.105447\n",
      "Train Epoch: 10 [4480/6000 (74%)]\tLoss: 0.111342\n",
      "Train Epoch: 10 [4800/6000 (80%)]\tLoss: 0.105829\n",
      "Train Epoch: 10 [5120/6000 (85%)]\tLoss: 0.101591\n",
      "Train Epoch: 10 [5440/6000 (90%)]\tLoss: 0.097667\n",
      "Train Epoch: 10 [5760/6000 (96%)]\tLoss: 0.106068\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 10 model saved!\n",
      "Train Epoch: 11 [320/6000 (5%)]\tLoss: 0.106647\n",
      "Train Epoch: 11 [640/6000 (11%)]\tLoss: 0.114937\n",
      "Train Epoch: 11 [960/6000 (16%)]\tLoss: 0.110978\n",
      "Train Epoch: 11 [1280/6000 (21%)]\tLoss: 0.095938\n",
      "Train Epoch: 11 [1600/6000 (27%)]\tLoss: 0.100871\n",
      "Train Epoch: 11 [1920/6000 (32%)]\tLoss: 0.115200\n",
      "Train Epoch: 11 [2240/6000 (37%)]\tLoss: 0.091816\n",
      "Train Epoch: 11 [2560/6000 (43%)]\tLoss: 0.101294\n",
      "Train Epoch: 11 [2880/6000 (48%)]\tLoss: 0.112941\n",
      "Train Epoch: 11 [3200/6000 (53%)]\tLoss: 0.112780\n",
      "Train Epoch: 11 [3520/6000 (59%)]\tLoss: 0.107332\n",
      "Train Epoch: 11 [3840/6000 (64%)]\tLoss: 0.102479\n",
      "Train Epoch: 11 [4160/6000 (69%)]\tLoss: 0.102017\n",
      "Train Epoch: 11 [4480/6000 (74%)]\tLoss: 0.125180\n",
      "Train Epoch: 11 [4800/6000 (80%)]\tLoss: 0.113495\n",
      "Train Epoch: 11 [5120/6000 (85%)]\tLoss: 0.110266\n",
      "Train Epoch: 11 [5440/6000 (90%)]\tLoss: 0.107122\n",
      "Train Epoch: 11 [5760/6000 (96%)]\tLoss: 0.116773\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 11 model saved!\n",
      "Train Epoch: 12 [320/6000 (5%)]\tLoss: 0.103323\n",
      "Train Epoch: 12 [640/6000 (11%)]\tLoss: 0.093090\n",
      "Train Epoch: 12 [960/6000 (16%)]\tLoss: 0.092418\n",
      "Train Epoch: 12 [1280/6000 (21%)]\tLoss: 0.105740\n",
      "Train Epoch: 12 [1600/6000 (27%)]\tLoss: 0.095521\n",
      "Train Epoch: 12 [1920/6000 (32%)]\tLoss: 0.096777\n",
      "Train Epoch: 12 [2240/6000 (37%)]\tLoss: 0.104035\n",
      "Train Epoch: 12 [2560/6000 (43%)]\tLoss: 0.105154\n",
      "Train Epoch: 12 [2880/6000 (48%)]\tLoss: 0.115713\n",
      "Train Epoch: 12 [3200/6000 (53%)]\tLoss: 0.106406\n",
      "Train Epoch: 12 [3520/6000 (59%)]\tLoss: 0.100866\n",
      "Train Epoch: 12 [3840/6000 (64%)]\tLoss: 0.108731\n",
      "Train Epoch: 12 [4160/6000 (69%)]\tLoss: 0.112067\n",
      "Train Epoch: 12 [4480/6000 (74%)]\tLoss: 0.090735\n",
      "Train Epoch: 12 [4800/6000 (80%)]\tLoss: 0.107974\n",
      "Train Epoch: 12 [5120/6000 (85%)]\tLoss: 0.101976\n",
      "Train Epoch: 12 [5440/6000 (90%)]\tLoss: 0.115350\n",
      "Train Epoch: 12 [5760/6000 (96%)]\tLoss: 0.100595\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 12 model saved!\n",
      "Train Epoch: 13 [320/6000 (5%)]\tLoss: 0.103731\n",
      "Train Epoch: 13 [640/6000 (11%)]\tLoss: 0.094479\n",
      "Train Epoch: 13 [960/6000 (16%)]\tLoss: 0.097909\n",
      "Train Epoch: 13 [1280/6000 (21%)]\tLoss: 0.104213\n",
      "Train Epoch: 13 [1600/6000 (27%)]\tLoss: 0.100350\n",
      "Train Epoch: 13 [1920/6000 (32%)]\tLoss: 0.104003\n",
      "Train Epoch: 13 [2240/6000 (37%)]\tLoss: 0.095948\n",
      "Train Epoch: 13 [2560/6000 (43%)]\tLoss: 0.097953\n",
      "Train Epoch: 13 [2880/6000 (48%)]\tLoss: 0.107842\n",
      "Train Epoch: 13 [3200/6000 (53%)]\tLoss: 0.102824\n",
      "Train Epoch: 13 [3520/6000 (59%)]\tLoss: 0.101102\n",
      "Train Epoch: 13 [3840/6000 (64%)]\tLoss: 0.099138\n",
      "Train Epoch: 13 [4160/6000 (69%)]\tLoss: 0.107554\n",
      "Train Epoch: 13 [4480/6000 (74%)]\tLoss: 0.090284\n",
      "Train Epoch: 13 [4800/6000 (80%)]\tLoss: 0.099282\n",
      "Train Epoch: 13 [5120/6000 (85%)]\tLoss: 0.099207\n",
      "Train Epoch: 13 [5440/6000 (90%)]\tLoss: 0.100991\n",
      "Train Epoch: 13 [5760/6000 (96%)]\tLoss: 0.114088\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 13 model saved!\n",
      "Train Epoch: 14 [320/6000 (5%)]\tLoss: 0.096194\n",
      "Train Epoch: 14 [640/6000 (11%)]\tLoss: 0.094713\n",
      "Train Epoch: 14 [960/6000 (16%)]\tLoss: 0.110965\n",
      "Train Epoch: 14 [1280/6000 (21%)]\tLoss: 0.109442\n",
      "Train Epoch: 14 [1600/6000 (27%)]\tLoss: 0.114462\n",
      "Train Epoch: 14 [1920/6000 (32%)]\tLoss: 0.111054\n",
      "Train Epoch: 14 [2240/6000 (37%)]\tLoss: 0.094189\n",
      "Train Epoch: 14 [2560/6000 (43%)]\tLoss: 0.096244\n",
      "Train Epoch: 14 [2880/6000 (48%)]\tLoss: 0.112802\n",
      "Train Epoch: 14 [3200/6000 (53%)]\tLoss: 0.099506\n",
      "Train Epoch: 14 [3520/6000 (59%)]\tLoss: 0.103751\n",
      "Train Epoch: 14 [3840/6000 (64%)]\tLoss: 0.086565\n",
      "Train Epoch: 14 [4160/6000 (69%)]\tLoss: 0.105315\n",
      "Train Epoch: 14 [4480/6000 (74%)]\tLoss: 0.104955\n",
      "Train Epoch: 14 [4800/6000 (80%)]\tLoss: 0.093387\n",
      "Train Epoch: 14 [5120/6000 (85%)]\tLoss: 0.111014\n",
      "Train Epoch: 14 [5440/6000 (90%)]\tLoss: 0.102140\n",
      "Train Epoch: 14 [5760/6000 (96%)]\tLoss: 0.107975\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 14 model saved!\n",
      "Train Epoch: 15 [320/6000 (5%)]\tLoss: 0.110626\n",
      "Train Epoch: 15 [640/6000 (11%)]\tLoss: 0.098438\n",
      "Train Epoch: 15 [960/6000 (16%)]\tLoss: 0.097547\n",
      "Train Epoch: 15 [1280/6000 (21%)]\tLoss: 0.104342\n",
      "Train Epoch: 15 [1600/6000 (27%)]\tLoss: 0.101888\n",
      "Train Epoch: 15 [1920/6000 (32%)]\tLoss: 0.093422\n",
      "Train Epoch: 15 [2240/6000 (37%)]\tLoss: 0.108574\n",
      "Train Epoch: 15 [2560/6000 (43%)]\tLoss: 0.095385\n",
      "Train Epoch: 15 [2880/6000 (48%)]\tLoss: 0.094481\n",
      "Train Epoch: 15 [3200/6000 (53%)]\tLoss: 0.096598\n",
      "Train Epoch: 15 [3520/6000 (59%)]\tLoss: 0.112694\n",
      "Train Epoch: 15 [3840/6000 (64%)]\tLoss: 0.105399\n",
      "Train Epoch: 15 [4160/6000 (69%)]\tLoss: 0.110278\n",
      "Train Epoch: 15 [4480/6000 (74%)]\tLoss: 0.089448\n",
      "Train Epoch: 15 [4800/6000 (80%)]\tLoss: 0.097264\n",
      "Train Epoch: 15 [5120/6000 (85%)]\tLoss: 0.119361\n",
      "Train Epoch: 15 [5440/6000 (90%)]\tLoss: 0.096587\n",
      "Train Epoch: 15 [5760/6000 (96%)]\tLoss: 0.099520\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 15 model saved!\n",
      "Train Epoch: 16 [320/6000 (5%)]\tLoss: 0.113865\n",
      "Train Epoch: 16 [640/6000 (11%)]\tLoss: 0.105194\n",
      "Train Epoch: 16 [960/6000 (16%)]\tLoss: 0.107032\n",
      "Train Epoch: 16 [1280/6000 (21%)]\tLoss: 0.090500\n",
      "Train Epoch: 16 [1600/6000 (27%)]\tLoss: 0.107419\n",
      "Train Epoch: 16 [1920/6000 (32%)]\tLoss: 0.096484\n",
      "Train Epoch: 16 [2240/6000 (37%)]\tLoss: 0.102904\n",
      "Train Epoch: 16 [2560/6000 (43%)]\tLoss: 0.095602\n",
      "Train Epoch: 16 [2880/6000 (48%)]\tLoss: 0.109108\n",
      "Train Epoch: 16 [3200/6000 (53%)]\tLoss: 0.108303\n",
      "Train Epoch: 16 [3520/6000 (59%)]\tLoss: 0.095089\n",
      "Train Epoch: 16 [3840/6000 (64%)]\tLoss: 0.090435\n",
      "Train Epoch: 16 [4160/6000 (69%)]\tLoss: 0.100983\n",
      "Train Epoch: 16 [4480/6000 (74%)]\tLoss: 0.082879\n",
      "Train Epoch: 16 [4800/6000 (80%)]\tLoss: 0.104492\n",
      "Train Epoch: 16 [5120/6000 (85%)]\tLoss: 0.100022\n",
      "Train Epoch: 16 [5440/6000 (90%)]\tLoss: 0.107982\n",
      "Train Epoch: 16 [5760/6000 (96%)]\tLoss: 0.100948\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 16 model saved!\n",
      "Train Epoch: 17 [320/6000 (5%)]\tLoss: 0.096904\n",
      "Train Epoch: 17 [640/6000 (11%)]\tLoss: 0.098481\n",
      "Train Epoch: 17 [960/6000 (16%)]\tLoss: 0.111767\n",
      "Train Epoch: 17 [1280/6000 (21%)]\tLoss: 0.106957\n",
      "Train Epoch: 17 [1600/6000 (27%)]\tLoss: 0.093033\n",
      "Train Epoch: 17 [1920/6000 (32%)]\tLoss: 0.095760\n",
      "Train Epoch: 17 [2240/6000 (37%)]\tLoss: 0.080936\n",
      "Train Epoch: 17 [2560/6000 (43%)]\tLoss: 0.109010\n",
      "Train Epoch: 17 [2880/6000 (48%)]\tLoss: 0.095423\n",
      "Train Epoch: 17 [3200/6000 (53%)]\tLoss: 0.111086\n",
      "Train Epoch: 17 [3520/6000 (59%)]\tLoss: 0.092825\n",
      "Train Epoch: 17 [3840/6000 (64%)]\tLoss: 0.111207\n",
      "Train Epoch: 17 [4160/6000 (69%)]\tLoss: 0.103850\n",
      "Train Epoch: 17 [4480/6000 (74%)]\tLoss: 0.104956\n",
      "Train Epoch: 17 [4800/6000 (80%)]\tLoss: 0.096351\n",
      "Train Epoch: 17 [5120/6000 (85%)]\tLoss: 0.097594\n",
      "Train Epoch: 17 [5440/6000 (90%)]\tLoss: 0.091877\n",
      "Train Epoch: 17 [5760/6000 (96%)]\tLoss: 0.089677\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 17 model saved!\n",
      "Train Epoch: 18 [320/6000 (5%)]\tLoss: 0.094414\n",
      "Train Epoch: 18 [640/6000 (11%)]\tLoss: 0.088315\n",
      "Train Epoch: 18 [960/6000 (16%)]\tLoss: 0.098503\n",
      "Train Epoch: 18 [1280/6000 (21%)]\tLoss: 0.099713\n",
      "Train Epoch: 18 [1600/6000 (27%)]\tLoss: 0.114548\n",
      "Train Epoch: 18 [1920/6000 (32%)]\tLoss: 0.097339\n",
      "Train Epoch: 18 [2240/6000 (37%)]\tLoss: 0.098915\n",
      "Train Epoch: 18 [2560/6000 (43%)]\tLoss: 0.103464\n",
      "Train Epoch: 18 [2880/6000 (48%)]\tLoss: 0.104466\n",
      "Train Epoch: 18 [3200/6000 (53%)]\tLoss: 0.084401\n",
      "Train Epoch: 18 [3520/6000 (59%)]\tLoss: 0.101808\n",
      "Train Epoch: 18 [3840/6000 (64%)]\tLoss: 0.089645\n",
      "Train Epoch: 18 [4160/6000 (69%)]\tLoss: 0.113001\n",
      "Train Epoch: 18 [4480/6000 (74%)]\tLoss: 0.104664\n",
      "Train Epoch: 18 [4800/6000 (80%)]\tLoss: 0.097660\n",
      "Train Epoch: 18 [5120/6000 (85%)]\tLoss: 0.101226\n",
      "Train Epoch: 18 [5440/6000 (90%)]\tLoss: 0.102014\n",
      "Train Epoch: 18 [5760/6000 (96%)]\tLoss: 0.106006\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 18 model saved!\n",
      "Train Epoch: 19 [320/6000 (5%)]\tLoss: 0.090328\n",
      "Train Epoch: 19 [640/6000 (11%)]\tLoss: 0.089724\n",
      "Train Epoch: 19 [960/6000 (16%)]\tLoss: 0.099861\n",
      "Train Epoch: 19 [1280/6000 (21%)]\tLoss: 0.109436\n",
      "Train Epoch: 19 [1600/6000 (27%)]\tLoss: 0.105947\n",
      "Train Epoch: 19 [1920/6000 (32%)]\tLoss: 0.109183\n",
      "Train Epoch: 19 [2240/6000 (37%)]\tLoss: 0.106414\n",
      "Train Epoch: 19 [2560/6000 (43%)]\tLoss: 0.110540\n",
      "Train Epoch: 19 [2880/6000 (48%)]\tLoss: 0.100871\n",
      "Train Epoch: 19 [3200/6000 (53%)]\tLoss: 0.085225\n",
      "Train Epoch: 19 [3520/6000 (59%)]\tLoss: 0.089409\n",
      "Train Epoch: 19 [3840/6000 (64%)]\tLoss: 0.093449\n",
      "Train Epoch: 19 [4160/6000 (69%)]\tLoss: 0.092527\n",
      "Train Epoch: 19 [4480/6000 (74%)]\tLoss: 0.091967\n",
      "Train Epoch: 19 [4800/6000 (80%)]\tLoss: 0.088275\n",
      "Train Epoch: 19 [5120/6000 (85%)]\tLoss: 0.119052\n",
      "Train Epoch: 19 [5440/6000 (90%)]\tLoss: 0.106866\n",
      "Train Epoch: 19 [5760/6000 (96%)]\tLoss: 0.102231\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 19 model saved!\n",
      "Train Epoch: 20 [320/6000 (5%)]\tLoss: 0.089328\n",
      "Train Epoch: 20 [640/6000 (11%)]\tLoss: 0.101578\n",
      "Train Epoch: 20 [960/6000 (16%)]\tLoss: 0.090721\n",
      "Train Epoch: 20 [1280/6000 (21%)]\tLoss: 0.095001\n",
      "Train Epoch: 20 [1600/6000 (27%)]\tLoss: 0.111983\n",
      "Train Epoch: 20 [1920/6000 (32%)]\tLoss: 0.104484\n",
      "Train Epoch: 20 [2240/6000 (37%)]\tLoss: 0.091611\n",
      "Train Epoch: 20 [2560/6000 (43%)]\tLoss: 0.102880\n",
      "Train Epoch: 20 [2880/6000 (48%)]\tLoss: 0.082717\n",
      "Train Epoch: 20 [3200/6000 (53%)]\tLoss: 0.102486\n",
      "Train Epoch: 20 [3520/6000 (59%)]\tLoss: 0.098327\n",
      "Train Epoch: 20 [3840/6000 (64%)]\tLoss: 0.086864\n",
      "Train Epoch: 20 [4160/6000 (69%)]\tLoss: 0.092603\n",
      "Train Epoch: 20 [4480/6000 (74%)]\tLoss: 0.101964\n",
      "Train Epoch: 20 [4800/6000 (80%)]\tLoss: 0.105362\n",
      "Train Epoch: 20 [5120/6000 (85%)]\tLoss: 0.099110\n",
      "Train Epoch: 20 [5440/6000 (90%)]\tLoss: 0.095069\n",
      "Train Epoch: 20 [5760/6000 (96%)]\tLoss: 0.104705\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 20 model saved!\n",
      "Train Epoch: 21 [320/6000 (5%)]\tLoss: 0.102075\n",
      "Train Epoch: 21 [640/6000 (11%)]\tLoss: 0.086422\n",
      "Train Epoch: 21 [960/6000 (16%)]\tLoss: 0.099606\n",
      "Train Epoch: 21 [1280/6000 (21%)]\tLoss: 0.098134\n",
      "Train Epoch: 21 [1600/6000 (27%)]\tLoss: 0.096893\n",
      "Train Epoch: 21 [1920/6000 (32%)]\tLoss: 0.091995\n",
      "Train Epoch: 21 [2240/6000 (37%)]\tLoss: 0.079907\n",
      "Train Epoch: 21 [2560/6000 (43%)]\tLoss: 0.109583\n",
      "Train Epoch: 21 [2880/6000 (48%)]\tLoss: 0.092722\n",
      "Train Epoch: 21 [3200/6000 (53%)]\tLoss: 0.099607\n",
      "Train Epoch: 21 [3520/6000 (59%)]\tLoss: 0.095771\n",
      "Train Epoch: 21 [3840/6000 (64%)]\tLoss: 0.094728\n",
      "Train Epoch: 21 [4160/6000 (69%)]\tLoss: 0.091874\n",
      "Train Epoch: 21 [4480/6000 (74%)]\tLoss: 0.093643\n",
      "Train Epoch: 21 [4800/6000 (80%)]\tLoss: 0.094374\n",
      "Train Epoch: 21 [5120/6000 (85%)]\tLoss: 0.082840\n",
      "Train Epoch: 21 [5440/6000 (90%)]\tLoss: 0.084281\n",
      "Train Epoch: 21 [5760/6000 (96%)]\tLoss: 0.100168\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 21 model saved!\n",
      "Train Epoch: 22 [320/6000 (5%)]\tLoss: 0.089481\n",
      "Train Epoch: 22 [640/6000 (11%)]\tLoss: 0.090623\n",
      "Train Epoch: 22 [960/6000 (16%)]\tLoss: 0.100500\n",
      "Train Epoch: 22 [1280/6000 (21%)]\tLoss: 0.096800\n",
      "Train Epoch: 22 [1600/6000 (27%)]\tLoss: 0.094254\n",
      "Train Epoch: 22 [1920/6000 (32%)]\tLoss: 0.095770\n",
      "Train Epoch: 22 [2240/6000 (37%)]\tLoss: 0.110161\n",
      "Train Epoch: 22 [2560/6000 (43%)]\tLoss: 0.096122\n",
      "Train Epoch: 22 [2880/6000 (48%)]\tLoss: 0.105267\n",
      "Train Epoch: 22 [3200/6000 (53%)]\tLoss: 0.084761\n",
      "Train Epoch: 22 [3520/6000 (59%)]\tLoss: 0.113942\n",
      "Train Epoch: 22 [3840/6000 (64%)]\tLoss: 0.096771\n",
      "Train Epoch: 22 [4160/6000 (69%)]\tLoss: 0.097121\n",
      "Train Epoch: 22 [4480/6000 (74%)]\tLoss: 0.109122\n",
      "Train Epoch: 22 [4800/6000 (80%)]\tLoss: 0.101681\n",
      "Train Epoch: 22 [5120/6000 (85%)]\tLoss: 0.096129\n",
      "Train Epoch: 22 [5440/6000 (90%)]\tLoss: 0.095353\n",
      "Train Epoch: 22 [5760/6000 (96%)]\tLoss: 0.090079\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 22 model saved!\n",
      "Train Epoch: 23 [320/6000 (5%)]\tLoss: 0.101723\n",
      "Train Epoch: 23 [640/6000 (11%)]\tLoss: 0.090197\n",
      "Train Epoch: 23 [960/6000 (16%)]\tLoss: 0.096682\n",
      "Train Epoch: 23 [1280/6000 (21%)]\tLoss: 0.091186\n",
      "Train Epoch: 23 [1600/6000 (27%)]\tLoss: 0.093133\n",
      "Train Epoch: 23 [1920/6000 (32%)]\tLoss: 0.102876\n",
      "Train Epoch: 23 [2240/6000 (37%)]\tLoss: 0.111114\n",
      "Train Epoch: 23 [2560/6000 (43%)]\tLoss: 0.100498\n",
      "Train Epoch: 23 [2880/6000 (48%)]\tLoss: 0.095898\n",
      "Train Epoch: 23 [3200/6000 (53%)]\tLoss: 0.088379\n",
      "Train Epoch: 23 [3520/6000 (59%)]\tLoss: 0.101527\n",
      "Train Epoch: 23 [3840/6000 (64%)]\tLoss: 0.093832\n",
      "Train Epoch: 23 [4160/6000 (69%)]\tLoss: 0.093151\n",
      "Train Epoch: 23 [4480/6000 (74%)]\tLoss: 0.083651\n",
      "Train Epoch: 23 [4800/6000 (80%)]\tLoss: 0.103436\n",
      "Train Epoch: 23 [5120/6000 (85%)]\tLoss: 0.087720\n",
      "Train Epoch: 23 [5440/6000 (90%)]\tLoss: 0.092864\n",
      "Train Epoch: 23 [5760/6000 (96%)]\tLoss: 0.086220\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 23 model saved!\n",
      "Train Epoch: 24 [320/6000 (5%)]\tLoss: 0.090379\n",
      "Train Epoch: 24 [640/6000 (11%)]\tLoss: 0.084512\n",
      "Train Epoch: 24 [960/6000 (16%)]\tLoss: 0.089000\n",
      "Train Epoch: 24 [1280/6000 (21%)]\tLoss: 0.089786\n",
      "Train Epoch: 24 [1600/6000 (27%)]\tLoss: 0.094263\n",
      "Train Epoch: 24 [1920/6000 (32%)]\tLoss: 0.087430\n",
      "Train Epoch: 24 [2240/6000 (37%)]\tLoss: 0.101069\n",
      "Train Epoch: 24 [2560/6000 (43%)]\tLoss: 0.092997\n",
      "Train Epoch: 24 [2880/6000 (48%)]\tLoss: 0.095267\n",
      "Train Epoch: 24 [3200/6000 (53%)]\tLoss: 0.099473\n",
      "Train Epoch: 24 [3520/6000 (59%)]\tLoss: 0.103827\n",
      "Train Epoch: 24 [3840/6000 (64%)]\tLoss: 0.090086\n",
      "Train Epoch: 24 [4160/6000 (69%)]\tLoss: 0.091743\n",
      "Train Epoch: 24 [4480/6000 (74%)]\tLoss: 0.087714\n",
      "Train Epoch: 24 [4800/6000 (80%)]\tLoss: 0.098968\n",
      "Train Epoch: 24 [5120/6000 (85%)]\tLoss: 0.093304\n",
      "Train Epoch: 24 [5440/6000 (90%)]\tLoss: 0.090937\n",
      "Train Epoch: 24 [5760/6000 (96%)]\tLoss: 0.090828\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 24 model saved!\n",
      "Train Epoch: 25 [320/6000 (5%)]\tLoss: 0.099115\n",
      "Train Epoch: 25 [640/6000 (11%)]\tLoss: 0.090209\n",
      "Train Epoch: 25 [960/6000 (16%)]\tLoss: 0.096718\n",
      "Train Epoch: 25 [1280/6000 (21%)]\tLoss: 0.098581\n",
      "Train Epoch: 25 [1600/6000 (27%)]\tLoss: 0.094917\n",
      "Train Epoch: 25 [1920/6000 (32%)]\tLoss: 0.090303\n",
      "Train Epoch: 25 [2240/6000 (37%)]\tLoss: 0.093052\n",
      "Train Epoch: 25 [2560/6000 (43%)]\tLoss: 0.092922\n",
      "Train Epoch: 25 [2880/6000 (48%)]\tLoss: 0.096335\n",
      "Train Epoch: 25 [3200/6000 (53%)]\tLoss: 0.097393\n",
      "Train Epoch: 25 [3520/6000 (59%)]\tLoss: 0.078551\n",
      "Train Epoch: 25 [3840/6000 (64%)]\tLoss: 0.103391\n",
      "Train Epoch: 25 [4160/6000 (69%)]\tLoss: 0.090039\n",
      "Train Epoch: 25 [4480/6000 (74%)]\tLoss: 0.086920\n",
      "Train Epoch: 25 [4800/6000 (80%)]\tLoss: 0.092837\n",
      "Train Epoch: 25 [5120/6000 (85%)]\tLoss: 0.087338\n",
      "Train Epoch: 25 [5440/6000 (90%)]\tLoss: 0.107280\n",
      "Train Epoch: 25 [5760/6000 (96%)]\tLoss: 0.097413\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 25 model saved!\n",
      "Train Epoch: 26 [320/6000 (5%)]\tLoss: 0.088091\n",
      "Train Epoch: 26 [640/6000 (11%)]\tLoss: 0.085194\n",
      "Train Epoch: 26 [960/6000 (16%)]\tLoss: 0.085511\n",
      "Train Epoch: 26 [1280/6000 (21%)]\tLoss: 0.091248\n",
      "Train Epoch: 26 [1600/6000 (27%)]\tLoss: 0.106701\n",
      "Train Epoch: 26 [1920/6000 (32%)]\tLoss: 0.097515\n",
      "Train Epoch: 26 [2240/6000 (37%)]\tLoss: 0.094618\n",
      "Train Epoch: 26 [2560/6000 (43%)]\tLoss: 0.094066\n",
      "Train Epoch: 26 [2880/6000 (48%)]\tLoss: 0.098472\n",
      "Train Epoch: 26 [3200/6000 (53%)]\tLoss: 0.096298\n",
      "Train Epoch: 26 [3520/6000 (59%)]\tLoss: 0.103694\n",
      "Train Epoch: 26 [3840/6000 (64%)]\tLoss: 0.090065\n",
      "Train Epoch: 26 [4160/6000 (69%)]\tLoss: 0.075880\n",
      "Train Epoch: 26 [4480/6000 (74%)]\tLoss: 0.096794\n",
      "Train Epoch: 26 [4800/6000 (80%)]\tLoss: 0.099127\n",
      "Train Epoch: 26 [5120/6000 (85%)]\tLoss: 0.098794\n",
      "Train Epoch: 26 [5440/6000 (90%)]\tLoss: 0.101459\n",
      "Train Epoch: 26 [5760/6000 (96%)]\tLoss: 0.091396\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 26 model saved!\n",
      "Train Epoch: 27 [320/6000 (5%)]\tLoss: 0.101902\n",
      "Train Epoch: 27 [640/6000 (11%)]\tLoss: 0.098920\n",
      "Train Epoch: 27 [960/6000 (16%)]\tLoss: 0.099023\n",
      "Train Epoch: 27 [1280/6000 (21%)]\tLoss: 0.098912\n",
      "Train Epoch: 27 [1600/6000 (27%)]\tLoss: 0.088469\n",
      "Train Epoch: 27 [1920/6000 (32%)]\tLoss: 0.106614\n",
      "Train Epoch: 27 [2240/6000 (37%)]\tLoss: 0.083422\n",
      "Train Epoch: 27 [2560/6000 (43%)]\tLoss: 0.102706\n",
      "Train Epoch: 27 [2880/6000 (48%)]\tLoss: 0.104538\n",
      "Train Epoch: 27 [3200/6000 (53%)]\tLoss: 0.090876\n",
      "Train Epoch: 27 [3520/6000 (59%)]\tLoss: 0.095242\n",
      "Train Epoch: 27 [3840/6000 (64%)]\tLoss: 0.097461\n",
      "Train Epoch: 27 [4160/6000 (69%)]\tLoss: 0.094240\n",
      "Train Epoch: 27 [4480/6000 (74%)]\tLoss: 0.086985\n",
      "Train Epoch: 27 [4800/6000 (80%)]\tLoss: 0.086666\n",
      "Train Epoch: 27 [5120/6000 (85%)]\tLoss: 0.084962\n",
      "Train Epoch: 27 [5440/6000 (90%)]\tLoss: 0.105401\n",
      "Train Epoch: 27 [5760/6000 (96%)]\tLoss: 0.092769\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 27 model saved!\n",
      "Train Epoch: 28 [320/6000 (5%)]\tLoss: 0.086792\n",
      "Train Epoch: 28 [640/6000 (11%)]\tLoss: 0.095110\n",
      "Train Epoch: 28 [960/6000 (16%)]\tLoss: 0.090793\n",
      "Train Epoch: 28 [1280/6000 (21%)]\tLoss: 0.091128\n",
      "Train Epoch: 28 [1600/6000 (27%)]\tLoss: 0.099602\n",
      "Train Epoch: 28 [1920/6000 (32%)]\tLoss: 0.103045\n",
      "Train Epoch: 28 [2240/6000 (37%)]\tLoss: 0.099226\n",
      "Train Epoch: 28 [2560/6000 (43%)]\tLoss: 0.095619\n",
      "Train Epoch: 28 [2880/6000 (48%)]\tLoss: 0.088946\n",
      "Train Epoch: 28 [3200/6000 (53%)]\tLoss: 0.090914\n",
      "Train Epoch: 28 [3520/6000 (59%)]\tLoss: 0.096849\n",
      "Train Epoch: 28 [3840/6000 (64%)]\tLoss: 0.086457\n",
      "Train Epoch: 28 [4160/6000 (69%)]\tLoss: 0.096184\n",
      "Train Epoch: 28 [4480/6000 (74%)]\tLoss: 0.096524\n",
      "Train Epoch: 28 [4800/6000 (80%)]\tLoss: 0.087698\n",
      "Train Epoch: 28 [5120/6000 (85%)]\tLoss: 0.091237\n",
      "Train Epoch: 28 [5440/6000 (90%)]\tLoss: 0.087735\n",
      "Train Epoch: 28 [5760/6000 (96%)]\tLoss: 0.085857\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 28 model saved!\n",
      "Train Epoch: 29 [320/6000 (5%)]\tLoss: 0.082767\n",
      "Train Epoch: 29 [640/6000 (11%)]\tLoss: 0.093976\n",
      "Train Epoch: 29 [960/6000 (16%)]\tLoss: 0.104600\n",
      "Train Epoch: 29 [1280/6000 (21%)]\tLoss: 0.081136\n",
      "Train Epoch: 29 [1600/6000 (27%)]\tLoss: 0.110460\n",
      "Train Epoch: 29 [1920/6000 (32%)]\tLoss: 0.096569\n",
      "Train Epoch: 29 [2240/6000 (37%)]\tLoss: 0.102102\n",
      "Train Epoch: 29 [2560/6000 (43%)]\tLoss: 0.093513\n",
      "Train Epoch: 29 [2880/6000 (48%)]\tLoss: 0.087376\n",
      "Train Epoch: 29 [3200/6000 (53%)]\tLoss: 0.104152\n",
      "Train Epoch: 29 [3520/6000 (59%)]\tLoss: 0.085869\n",
      "Train Epoch: 29 [3840/6000 (64%)]\tLoss: 0.105994\n",
      "Train Epoch: 29 [4160/6000 (69%)]\tLoss: 0.090645\n",
      "Train Epoch: 29 [4480/6000 (74%)]\tLoss: 0.097029\n",
      "Train Epoch: 29 [4800/6000 (80%)]\tLoss: 0.090007\n",
      "Train Epoch: 29 [5120/6000 (85%)]\tLoss: 0.103305\n",
      "Train Epoch: 29 [5440/6000 (90%)]\tLoss: 0.092809\n",
      "Train Epoch: 29 [5760/6000 (96%)]\tLoss: 0.104835\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 29 model saved!\n",
      "Train Epoch: 30 [320/6000 (5%)]\tLoss: 0.102867\n",
      "Train Epoch: 30 [640/6000 (11%)]\tLoss: 0.102265\n",
      "Train Epoch: 30 [960/6000 (16%)]\tLoss: 0.100370\n",
      "Train Epoch: 30 [1280/6000 (21%)]\tLoss: 0.098083\n",
      "Train Epoch: 30 [1600/6000 (27%)]\tLoss: 0.097900\n",
      "Train Epoch: 30 [1920/6000 (32%)]\tLoss: 0.088593\n",
      "Train Epoch: 30 [2240/6000 (37%)]\tLoss: 0.099085\n",
      "Train Epoch: 30 [2560/6000 (43%)]\tLoss: 0.080926\n",
      "Train Epoch: 30 [2880/6000 (48%)]\tLoss: 0.087607\n",
      "Train Epoch: 30 [3200/6000 (53%)]\tLoss: 0.096795\n",
      "Train Epoch: 30 [3520/6000 (59%)]\tLoss: 0.095336\n",
      "Train Epoch: 30 [3840/6000 (64%)]\tLoss: 0.096922\n",
      "Train Epoch: 30 [4160/6000 (69%)]\tLoss: 0.103814\n",
      "Train Epoch: 30 [4480/6000 (74%)]\tLoss: 0.101889\n",
      "Train Epoch: 30 [4800/6000 (80%)]\tLoss: 0.101549\n",
      "Train Epoch: 30 [5120/6000 (85%)]\tLoss: 0.090149\n",
      "Train Epoch: 30 [5440/6000 (90%)]\tLoss: 0.082899\n",
      "Train Epoch: 30 [5760/6000 (96%)]\tLoss: 0.093146\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 30 model saved!\n",
      "Train Epoch: 31 [320/6000 (5%)]\tLoss: 0.077084\n",
      "Train Epoch: 31 [640/6000 (11%)]\tLoss: 0.091645\n",
      "Train Epoch: 31 [960/6000 (16%)]\tLoss: 0.087481\n",
      "Train Epoch: 31 [1280/6000 (21%)]\tLoss: 0.108005\n",
      "Train Epoch: 31 [1600/6000 (27%)]\tLoss: 0.106523\n",
      "Train Epoch: 31 [1920/6000 (32%)]\tLoss: 0.105060\n",
      "Train Epoch: 31 [2240/6000 (37%)]\tLoss: 0.100919\n",
      "Train Epoch: 31 [2560/6000 (43%)]\tLoss: 0.102512\n",
      "Train Epoch: 31 [2880/6000 (48%)]\tLoss: 0.091369\n",
      "Train Epoch: 31 [3200/6000 (53%)]\tLoss: 0.082547\n",
      "Train Epoch: 31 [3520/6000 (59%)]\tLoss: 0.077518\n",
      "Train Epoch: 31 [3840/6000 (64%)]\tLoss: 0.091184\n",
      "Train Epoch: 31 [4160/6000 (69%)]\tLoss: 0.096959\n",
      "Train Epoch: 31 [4480/6000 (74%)]\tLoss: 0.091079\n",
      "Train Epoch: 31 [4800/6000 (80%)]\tLoss: 0.086353\n",
      "Train Epoch: 31 [5120/6000 (85%)]\tLoss: 0.085683\n",
      "Train Epoch: 31 [5440/6000 (90%)]\tLoss: 0.092730\n",
      "Train Epoch: 31 [5760/6000 (96%)]\tLoss: 0.087926\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 31 model saved!\n",
      "Train Epoch: 32 [320/6000 (5%)]\tLoss: 0.083892\n",
      "Train Epoch: 32 [640/6000 (11%)]\tLoss: 0.092965\n",
      "Train Epoch: 32 [960/6000 (16%)]\tLoss: 0.099055\n",
      "Train Epoch: 32 [1280/6000 (21%)]\tLoss: 0.084090\n",
      "Train Epoch: 32 [1600/6000 (27%)]\tLoss: 0.090364\n",
      "Train Epoch: 32 [1920/6000 (32%)]\tLoss: 0.086380\n",
      "Train Epoch: 32 [2240/6000 (37%)]\tLoss: 0.084606\n",
      "Train Epoch: 32 [2560/6000 (43%)]\tLoss: 0.085317\n",
      "Train Epoch: 32 [2880/6000 (48%)]\tLoss: 0.088608\n",
      "Train Epoch: 32 [3200/6000 (53%)]\tLoss: 0.092115\n",
      "Train Epoch: 32 [3520/6000 (59%)]\tLoss: 0.080996\n",
      "Train Epoch: 32 [3840/6000 (64%)]\tLoss: 0.099499\n",
      "Train Epoch: 32 [4160/6000 (69%)]\tLoss: 0.086843\n",
      "Train Epoch: 32 [4480/6000 (74%)]\tLoss: 0.100505\n",
      "Train Epoch: 32 [4800/6000 (80%)]\tLoss: 0.098233\n",
      "Train Epoch: 32 [5120/6000 (85%)]\tLoss: 0.088397\n",
      "Train Epoch: 32 [5440/6000 (90%)]\tLoss: 0.094464\n",
      "Train Epoch: 32 [5760/6000 (96%)]\tLoss: 0.100613\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 32 model saved!\n",
      "Train Epoch: 33 [320/6000 (5%)]\tLoss: 0.099513\n",
      "Train Epoch: 33 [640/6000 (11%)]\tLoss: 0.084916\n",
      "Train Epoch: 33 [960/6000 (16%)]\tLoss: 0.089688\n",
      "Train Epoch: 33 [1280/6000 (21%)]\tLoss: 0.101995\n",
      "Train Epoch: 33 [1600/6000 (27%)]\tLoss: 0.101242\n",
      "Train Epoch: 33 [1920/6000 (32%)]\tLoss: 0.079314\n",
      "Train Epoch: 33 [2240/6000 (37%)]\tLoss: 0.106283\n",
      "Train Epoch: 33 [2560/6000 (43%)]\tLoss: 0.087022\n",
      "Train Epoch: 33 [2880/6000 (48%)]\tLoss: 0.102995\n",
      "Train Epoch: 33 [3200/6000 (53%)]\tLoss: 0.094440\n",
      "Train Epoch: 33 [3520/6000 (59%)]\tLoss: 0.096517\n",
      "Train Epoch: 33 [3840/6000 (64%)]\tLoss: 0.095260\n",
      "Train Epoch: 33 [4160/6000 (69%)]\tLoss: 0.084028\n",
      "Train Epoch: 33 [4480/6000 (74%)]\tLoss: 0.096330\n",
      "Train Epoch: 33 [4800/6000 (80%)]\tLoss: 0.087125\n",
      "Train Epoch: 33 [5120/6000 (85%)]\tLoss: 0.103216\n",
      "Train Epoch: 33 [5440/6000 (90%)]\tLoss: 0.094824\n",
      "Train Epoch: 33 [5760/6000 (96%)]\tLoss: 0.089205\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 33 model saved!\n",
      "Train Epoch: 34 [320/6000 (5%)]\tLoss: 0.079598\n",
      "Train Epoch: 34 [640/6000 (11%)]\tLoss: 0.081437\n",
      "Train Epoch: 34 [960/6000 (16%)]\tLoss: 0.092556\n",
      "Train Epoch: 34 [1280/6000 (21%)]\tLoss: 0.080166\n",
      "Train Epoch: 34 [1600/6000 (27%)]\tLoss: 0.083987\n",
      "Train Epoch: 34 [1920/6000 (32%)]\tLoss: 0.090346\n",
      "Train Epoch: 34 [2240/6000 (37%)]\tLoss: 0.100719\n",
      "Train Epoch: 34 [2560/6000 (43%)]\tLoss: 0.095984\n",
      "Train Epoch: 34 [2880/6000 (48%)]\tLoss: 0.094030\n",
      "Train Epoch: 34 [3200/6000 (53%)]\tLoss: 0.091306\n",
      "Train Epoch: 34 [3520/6000 (59%)]\tLoss: 0.092624\n",
      "Train Epoch: 34 [3840/6000 (64%)]\tLoss: 0.087317\n",
      "Train Epoch: 34 [4160/6000 (69%)]\tLoss: 0.090229\n",
      "Train Epoch: 34 [4480/6000 (74%)]\tLoss: 0.113970\n",
      "Train Epoch: 34 [4800/6000 (80%)]\tLoss: 0.105505\n",
      "Train Epoch: 34 [5120/6000 (85%)]\tLoss: 0.093365\n",
      "Train Epoch: 34 [5440/6000 (90%)]\tLoss: 0.098493\n",
      "Train Epoch: 34 [5760/6000 (96%)]\tLoss: 0.084019\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 34 model saved!\n",
      "Train Epoch: 35 [320/6000 (5%)]\tLoss: 0.084971\n",
      "Train Epoch: 35 [640/6000 (11%)]\tLoss: 0.101743\n",
      "Train Epoch: 35 [960/6000 (16%)]\tLoss: 0.085768\n",
      "Train Epoch: 35 [1280/6000 (21%)]\tLoss: 0.082470\n",
      "Train Epoch: 35 [1600/6000 (27%)]\tLoss: 0.095108\n",
      "Train Epoch: 35 [1920/6000 (32%)]\tLoss: 0.102508\n",
      "Train Epoch: 35 [2240/6000 (37%)]\tLoss: 0.092370\n",
      "Train Epoch: 35 [2560/6000 (43%)]\tLoss: 0.083360\n",
      "Train Epoch: 35 [2880/6000 (48%)]\tLoss: 0.086214\n",
      "Train Epoch: 35 [3200/6000 (53%)]\tLoss: 0.079426\n",
      "Train Epoch: 35 [3520/6000 (59%)]\tLoss: 0.089169\n",
      "Train Epoch: 35 [3840/6000 (64%)]\tLoss: 0.091003\n",
      "Train Epoch: 35 [4160/6000 (69%)]\tLoss: 0.096808\n",
      "Train Epoch: 35 [4480/6000 (74%)]\tLoss: 0.079900\n",
      "Train Epoch: 35 [4800/6000 (80%)]\tLoss: 0.099129\n",
      "Train Epoch: 35 [5120/6000 (85%)]\tLoss: 0.095680\n",
      "Train Epoch: 35 [5440/6000 (90%)]\tLoss: 0.105917\n",
      "Train Epoch: 35 [5760/6000 (96%)]\tLoss: 0.093415\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 35 model saved!\n",
      "Train Epoch: 36 [320/6000 (5%)]\tLoss: 0.088362\n",
      "Train Epoch: 36 [640/6000 (11%)]\tLoss: 0.077920\n",
      "Train Epoch: 36 [960/6000 (16%)]\tLoss: 0.091463\n",
      "Train Epoch: 36 [1280/6000 (21%)]\tLoss: 0.101053\n",
      "Train Epoch: 36 [1600/6000 (27%)]\tLoss: 0.096498\n",
      "Train Epoch: 36 [1920/6000 (32%)]\tLoss: 0.089331\n",
      "Train Epoch: 36 [2240/6000 (37%)]\tLoss: 0.089202\n",
      "Train Epoch: 36 [2560/6000 (43%)]\tLoss: 0.086125\n",
      "Train Epoch: 36 [2880/6000 (48%)]\tLoss: 0.094716\n",
      "Train Epoch: 36 [3200/6000 (53%)]\tLoss: 0.096004\n",
      "Train Epoch: 36 [3520/6000 (59%)]\tLoss: 0.087818\n",
      "Train Epoch: 36 [3840/6000 (64%)]\tLoss: 0.098914\n",
      "Train Epoch: 36 [4160/6000 (69%)]\tLoss: 0.083485\n",
      "Train Epoch: 36 [4480/6000 (74%)]\tLoss: 0.108648\n",
      "Train Epoch: 36 [4800/6000 (80%)]\tLoss: 0.090908\n",
      "Train Epoch: 36 [5120/6000 (85%)]\tLoss: 0.111331\n",
      "Train Epoch: 36 [5440/6000 (90%)]\tLoss: 0.098619\n",
      "Train Epoch: 36 [5760/6000 (96%)]\tLoss: 0.095641\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 36 model saved!\n",
      "Train Epoch: 37 [320/6000 (5%)]\tLoss: 0.096244\n",
      "Train Epoch: 37 [640/6000 (11%)]\tLoss: 0.077009\n",
      "Train Epoch: 37 [960/6000 (16%)]\tLoss: 0.077107\n",
      "Train Epoch: 37 [1280/6000 (21%)]\tLoss: 0.082314\n",
      "Train Epoch: 37 [1600/6000 (27%)]\tLoss: 0.090043\n",
      "Train Epoch: 37 [1920/6000 (32%)]\tLoss: 0.077563\n",
      "Train Epoch: 37 [2240/6000 (37%)]\tLoss: 0.095160\n",
      "Train Epoch: 37 [2560/6000 (43%)]\tLoss: 0.079316\n",
      "Train Epoch: 37 [2880/6000 (48%)]\tLoss: 0.087570\n",
      "Train Epoch: 37 [3200/6000 (53%)]\tLoss: 0.090847\n",
      "Train Epoch: 37 [3520/6000 (59%)]\tLoss: 0.086664\n",
      "Train Epoch: 37 [3840/6000 (64%)]\tLoss: 0.096225\n",
      "Train Epoch: 37 [4160/6000 (69%)]\tLoss: 0.092448\n",
      "Train Epoch: 37 [4480/6000 (74%)]\tLoss: 0.089799\n",
      "Train Epoch: 37 [4800/6000 (80%)]\tLoss: 0.085195\n",
      "Train Epoch: 37 [5120/6000 (85%)]\tLoss: 0.087317\n",
      "Train Epoch: 37 [5440/6000 (90%)]\tLoss: 0.082771\n",
      "Train Epoch: 37 [5760/6000 (96%)]\tLoss: 0.086964\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 37 model saved!\n",
      "Train Epoch: 38 [320/6000 (5%)]\tLoss: 0.089969\n",
      "Train Epoch: 38 [640/6000 (11%)]\tLoss: 0.090022\n",
      "Train Epoch: 38 [960/6000 (16%)]\tLoss: 0.101049\n",
      "Train Epoch: 38 [1280/6000 (21%)]\tLoss: 0.085261\n",
      "Train Epoch: 38 [1600/6000 (27%)]\tLoss: 0.077671\n",
      "Train Epoch: 38 [1920/6000 (32%)]\tLoss: 0.094160\n",
      "Train Epoch: 38 [2240/6000 (37%)]\tLoss: 0.086455\n",
      "Train Epoch: 38 [2560/6000 (43%)]\tLoss: 0.105372\n",
      "Train Epoch: 38 [2880/6000 (48%)]\tLoss: 0.079693\n",
      "Train Epoch: 38 [3200/6000 (53%)]\tLoss: 0.095281\n",
      "Train Epoch: 38 [3520/6000 (59%)]\tLoss: 0.082129\n",
      "Train Epoch: 38 [3840/6000 (64%)]\tLoss: 0.080609\n",
      "Train Epoch: 38 [4160/6000 (69%)]\tLoss: 0.090188\n",
      "Train Epoch: 38 [4480/6000 (74%)]\tLoss: 0.095402\n",
      "Train Epoch: 38 [4800/6000 (80%)]\tLoss: 0.096203\n",
      "Train Epoch: 38 [5120/6000 (85%)]\tLoss: 0.087151\n",
      "Train Epoch: 38 [5440/6000 (90%)]\tLoss: 0.094176\n",
      "Train Epoch: 38 [5760/6000 (96%)]\tLoss: 0.093089\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 38 model saved!\n",
      "Train Epoch: 39 [320/6000 (5%)]\tLoss: 0.081263\n",
      "Train Epoch: 39 [640/6000 (11%)]\tLoss: 0.088565\n",
      "Train Epoch: 39 [960/6000 (16%)]\tLoss: 0.082903\n",
      "Train Epoch: 39 [1280/6000 (21%)]\tLoss: 0.091480\n",
      "Train Epoch: 39 [1600/6000 (27%)]\tLoss: 0.098297\n",
      "Train Epoch: 39 [1920/6000 (32%)]\tLoss: 0.094400\n",
      "Train Epoch: 39 [2240/6000 (37%)]\tLoss: 0.085017\n",
      "Train Epoch: 39 [2560/6000 (43%)]\tLoss: 0.100971\n",
      "Train Epoch: 39 [2880/6000 (48%)]\tLoss: 0.098193\n",
      "Train Epoch: 39 [3200/6000 (53%)]\tLoss: 0.099470\n",
      "Train Epoch: 39 [3520/6000 (59%)]\tLoss: 0.090127\n",
      "Train Epoch: 39 [3840/6000 (64%)]\tLoss: 0.083600\n",
      "Train Epoch: 39 [4160/6000 (69%)]\tLoss: 0.078269\n",
      "Train Epoch: 39 [4480/6000 (74%)]\tLoss: 0.087572\n",
      "Train Epoch: 39 [4800/6000 (80%)]\tLoss: 0.086989\n",
      "Train Epoch: 39 [5120/6000 (85%)]\tLoss: 0.099672\n",
      "Train Epoch: 39 [5440/6000 (90%)]\tLoss: 0.083182\n",
      "Train Epoch: 39 [5760/6000 (96%)]\tLoss: 0.081126\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 39 model saved!\n",
      "Train Epoch: 40 [320/6000 (5%)]\tLoss: 0.090419\n",
      "Train Epoch: 40 [640/6000 (11%)]\tLoss: 0.087970\n",
      "Train Epoch: 40 [960/6000 (16%)]\tLoss: 0.087115\n",
      "Train Epoch: 40 [1280/6000 (21%)]\tLoss: 0.085080\n",
      "Train Epoch: 40 [1600/6000 (27%)]\tLoss: 0.097020\n",
      "Train Epoch: 40 [1920/6000 (32%)]\tLoss: 0.086942\n",
      "Train Epoch: 40 [2240/6000 (37%)]\tLoss: 0.082396\n",
      "Train Epoch: 40 [2560/6000 (43%)]\tLoss: 0.081316\n",
      "Train Epoch: 40 [2880/6000 (48%)]\tLoss: 0.093669\n",
      "Train Epoch: 40 [3200/6000 (53%)]\tLoss: 0.098028\n",
      "Train Epoch: 40 [3520/6000 (59%)]\tLoss: 0.094809\n",
      "Train Epoch: 40 [3840/6000 (64%)]\tLoss: 0.083844\n",
      "Train Epoch: 40 [4160/6000 (69%)]\tLoss: 0.094790\n",
      "Train Epoch: 40 [4480/6000 (74%)]\tLoss: 0.077862\n",
      "Train Epoch: 40 [4800/6000 (80%)]\tLoss: 0.081525\n",
      "Train Epoch: 40 [5120/6000 (85%)]\tLoss: 0.079992\n",
      "Train Epoch: 40 [5440/6000 (90%)]\tLoss: 0.091976\n",
      "Train Epoch: 40 [5760/6000 (96%)]\tLoss: 0.081759\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0038, \n",
      "\n",
      "Epoch 40 model saved!\n",
      "Train Epoch: 41 [320/6000 (5%)]\tLoss: 0.091984\n",
      "Train Epoch: 41 [640/6000 (11%)]\tLoss: 0.091826\n",
      "Train Epoch: 41 [960/6000 (16%)]\tLoss: 0.091761\n",
      "Train Epoch: 41 [1280/6000 (21%)]\tLoss: 0.091045\n",
      "Train Epoch: 41 [1600/6000 (27%)]\tLoss: 0.074656\n",
      "Train Epoch: 41 [1920/6000 (32%)]\tLoss: 0.078769\n",
      "Train Epoch: 41 [2240/6000 (37%)]\tLoss: 0.089347\n",
      "Train Epoch: 41 [2560/6000 (43%)]\tLoss: 0.082293\n",
      "Train Epoch: 41 [2880/6000 (48%)]\tLoss: 0.097225\n",
      "Train Epoch: 41 [3200/6000 (53%)]\tLoss: 0.106673\n",
      "Train Epoch: 41 [3520/6000 (59%)]\tLoss: 0.083983\n",
      "Train Epoch: 41 [3840/6000 (64%)]\tLoss: 0.100084\n",
      "Train Epoch: 41 [4160/6000 (69%)]\tLoss: 0.090682\n",
      "Train Epoch: 41 [4480/6000 (74%)]\tLoss: 0.081479\n",
      "Train Epoch: 41 [4800/6000 (80%)]\tLoss: 0.078631\n",
      "Train Epoch: 41 [5120/6000 (85%)]\tLoss: 0.085525\n",
      "Train Epoch: 41 [5440/6000 (90%)]\tLoss: 0.081948\n",
      "Train Epoch: 41 [5760/6000 (96%)]\tLoss: 0.090789\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0038, \n",
      "\n",
      "Epoch 41 model saved!\n",
      "Train Epoch: 42 [320/6000 (5%)]\tLoss: 0.086677\n",
      "Train Epoch: 42 [640/6000 (11%)]\tLoss: 0.085749\n",
      "Train Epoch: 42 [960/6000 (16%)]\tLoss: 0.079543\n",
      "Train Epoch: 42 [1280/6000 (21%)]\tLoss: 0.090984\n",
      "Train Epoch: 42 [1600/6000 (27%)]\tLoss: 0.126748\n",
      "Train Epoch: 42 [1920/6000 (32%)]\tLoss: 0.086504\n",
      "Train Epoch: 42 [2240/6000 (37%)]\tLoss: 0.092515\n",
      "Train Epoch: 42 [2560/6000 (43%)]\tLoss: 0.096711\n",
      "Train Epoch: 42 [2880/6000 (48%)]\tLoss: 0.098879\n",
      "Train Epoch: 42 [3200/6000 (53%)]\tLoss: 0.090008\n",
      "Train Epoch: 42 [3520/6000 (59%)]\tLoss: 0.080798\n",
      "Train Epoch: 42 [3840/6000 (64%)]\tLoss: 0.083567\n",
      "Train Epoch: 42 [4160/6000 (69%)]\tLoss: 0.085762\n",
      "Train Epoch: 42 [4480/6000 (74%)]\tLoss: 0.097180\n",
      "Train Epoch: 42 [4800/6000 (80%)]\tLoss: 0.092078\n",
      "Train Epoch: 42 [5120/6000 (85%)]\tLoss: 0.087880\n",
      "Train Epoch: 42 [5440/6000 (90%)]\tLoss: 0.081094\n",
      "Train Epoch: 42 [5760/6000 (96%)]\tLoss: 0.090649\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 42 model saved!\n",
      "Train Epoch: 43 [320/6000 (5%)]\tLoss: 0.082671\n",
      "Train Epoch: 43 [640/6000 (11%)]\tLoss: 0.071439\n",
      "Train Epoch: 43 [960/6000 (16%)]\tLoss: 0.076099\n",
      "Train Epoch: 43 [1280/6000 (21%)]\tLoss: 0.087863\n",
      "Train Epoch: 43 [1600/6000 (27%)]\tLoss: 0.090051\n",
      "Train Epoch: 43 [1920/6000 (32%)]\tLoss: 0.083514\n",
      "Train Epoch: 43 [2240/6000 (37%)]\tLoss: 0.093002\n",
      "Train Epoch: 43 [2560/6000 (43%)]\tLoss: 0.081897\n",
      "Train Epoch: 43 [2880/6000 (48%)]\tLoss: 0.079367\n",
      "Train Epoch: 43 [3200/6000 (53%)]\tLoss: 0.086828\n",
      "Train Epoch: 43 [3520/6000 (59%)]\tLoss: 0.082615\n",
      "Train Epoch: 43 [3840/6000 (64%)]\tLoss: 0.083691\n",
      "Train Epoch: 43 [4160/6000 (69%)]\tLoss: 0.085386\n",
      "Train Epoch: 43 [4480/6000 (74%)]\tLoss: 0.084402\n",
      "Train Epoch: 43 [4800/6000 (80%)]\tLoss: 0.088959\n",
      "Train Epoch: 43 [5120/6000 (85%)]\tLoss: 0.091846\n",
      "Train Epoch: 43 [5440/6000 (90%)]\tLoss: 0.082225\n",
      "Train Epoch: 43 [5760/6000 (96%)]\tLoss: 0.081159\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0038, \n",
      "\n",
      "Epoch 43 model saved!\n",
      "Train Epoch: 44 [320/6000 (5%)]\tLoss: 0.082626\n",
      "Train Epoch: 44 [640/6000 (11%)]\tLoss: 0.089694\n",
      "Train Epoch: 44 [960/6000 (16%)]\tLoss: 0.083753\n",
      "Train Epoch: 44 [1280/6000 (21%)]\tLoss: 0.094489\n",
      "Train Epoch: 44 [1600/6000 (27%)]\tLoss: 0.085806\n",
      "Train Epoch: 44 [1920/6000 (32%)]\tLoss: 0.089512\n",
      "Train Epoch: 44 [2240/6000 (37%)]\tLoss: 0.091333\n",
      "Train Epoch: 44 [2560/6000 (43%)]\tLoss: 0.083457\n",
      "Train Epoch: 44 [2880/6000 (48%)]\tLoss: 0.089982\n",
      "Train Epoch: 44 [3200/6000 (53%)]\tLoss: 0.084318\n",
      "Train Epoch: 44 [3520/6000 (59%)]\tLoss: 0.100098\n",
      "Train Epoch: 44 [3840/6000 (64%)]\tLoss: 0.082550\n",
      "Train Epoch: 44 [4160/6000 (69%)]\tLoss: 0.092226\n",
      "Train Epoch: 44 [4480/6000 (74%)]\tLoss: 0.094480\n",
      "Train Epoch: 44 [4800/6000 (80%)]\tLoss: 0.076513\n",
      "Train Epoch: 44 [5120/6000 (85%)]\tLoss: 0.103605\n",
      "Train Epoch: 44 [5440/6000 (90%)]\tLoss: 0.086529\n",
      "Train Epoch: 44 [5760/6000 (96%)]\tLoss: 0.084990\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0040, \n",
      "\n",
      "Epoch 44 model saved!\n",
      "Train Epoch: 45 [320/6000 (5%)]\tLoss: 0.076050\n",
      "Train Epoch: 45 [640/6000 (11%)]\tLoss: 0.091207\n",
      "Train Epoch: 45 [960/6000 (16%)]\tLoss: 0.086829\n",
      "Train Epoch: 45 [1280/6000 (21%)]\tLoss: 0.097300\n",
      "Train Epoch: 45 [1600/6000 (27%)]\tLoss: 0.081893\n",
      "Train Epoch: 45 [1920/6000 (32%)]\tLoss: 0.084973\n",
      "Train Epoch: 45 [2240/6000 (37%)]\tLoss: 0.080101\n",
      "Train Epoch: 45 [2560/6000 (43%)]\tLoss: 0.087011\n",
      "Train Epoch: 45 [2880/6000 (48%)]\tLoss: 0.084660\n",
      "Train Epoch: 45 [3200/6000 (53%)]\tLoss: 0.106855\n",
      "Train Epoch: 45 [3520/6000 (59%)]\tLoss: 0.101265\n",
      "Train Epoch: 45 [3840/6000 (64%)]\tLoss: 0.081734\n",
      "Train Epoch: 45 [4160/6000 (69%)]\tLoss: 0.087818\n",
      "Train Epoch: 45 [4480/6000 (74%)]\tLoss: 0.081384\n",
      "Train Epoch: 45 [4800/6000 (80%)]\tLoss: 0.082267\n",
      "Train Epoch: 45 [5120/6000 (85%)]\tLoss: 0.094996\n",
      "Train Epoch: 45 [5440/6000 (90%)]\tLoss: 0.092827\n",
      "Train Epoch: 45 [5760/6000 (96%)]\tLoss: 0.082881\n",
      "\n",
      "Test set (2000 samples): Average loss: 0.0039, \n",
      "\n",
      "Epoch 45 model saved!\n",
      "Train Epoch: 46 [320/6000 (5%)]\tLoss: 0.084528\n",
      "Train Epoch: 46 [640/6000 (11%)]\tLoss: 0.081262\n",
      "Train Epoch: 46 [960/6000 (16%)]\tLoss: 0.088548\n",
      "Train Epoch: 46 [1280/6000 (21%)]\tLoss: 0.083591\n",
      "Train Epoch: 46 [1600/6000 (27%)]\tLoss: 0.094544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8e630891d57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# train, test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mepoch_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-f70dc6cfad54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mN_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m   \u001b[0;31m# counting total trained sample in one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# distribute data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/first-impressions/bin2/functions.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# (input) spatial images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/first-impressions/bin2/functions.py\u001b[0m in \u001b[0;36mread_images\u001b[0;34m(self, path, selected_folder, use_transform)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_folder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frame{}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs518/env/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m# PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    if __name__ == '__main__':\n",
    "        train_losses, train_scores = train(log_interval, resnet_encoder, device, train_loader, optimizer, epoch)\n",
    "        epoch_test_loss = validation(resnet_encoder, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    np.save('./CRNN_epoch_training_lossesMAE.npy', A)\n",
    "    np.save('./CRNN_epoch_test_lossMAE.npy', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'resnet_encoder_epoch43.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:07<00:00,  8.92it/s]\n"
     ]
    }
   ],
   "source": [
    "all_y_pred = CRNN_final_prediction(resnet_encoder, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((2000,6))\n",
    "k = 0\n",
    "for i in range(len(all_y_pred)):\n",
    "    for j in range(len(all_y_pred[i])):\n",
    "        batch_pred = all_y_pred[i].cpu()\n",
    "        preds[k] = batch_pred[j]\n",
    "        k +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61028981 0.58072871 0.61593825 0.71433711 0.63749462 0.66685444]\n",
      " [0.55970967 0.55023348 0.57460058 0.55090833 0.58938539 0.57664657]\n",
      " [0.45658562 0.41326034 0.54161274 0.48947003 0.46061429 0.49897346]\n",
      " ...\n",
      " [0.65526438 0.57461441 0.58798432 0.67181104 0.67036235 0.75111884]\n",
      " [0.47208357 0.42950732 0.54061741 0.55071718 0.49810818 0.56203741]\n",
      " [0.24775676 0.19362608 0.39594135 0.385773   0.28244293 0.30333051]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41121495 0.42990654 0.50549451 0.45631068 0.53125    0.46666667]\n",
      " [0.3271028  0.27102804 0.47252747 0.31067961 0.33333333 0.43333333]\n",
      " [0.4953271  0.35514019 0.52747253 0.67961165 0.39583333 0.45555556]\n",
      " ...\n",
      " [0.6728972  0.53271028 0.54945055 0.73786408 0.63541667 0.81111111]\n",
      " [0.57009346 0.40186916 0.65934066 0.55339806 0.53125    0.54444444]\n",
      " [0.41121495 0.28037383 0.50549451 0.48543689 0.35416667 0.37777778]]\n"
     ]
    }
   ],
   "source": [
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90467691 0.898808   0.90380771 0.90391984 0.89706602 0.90127052]\n",
      "0.9015915009204183\n"
     ]
    }
   ],
   "source": [
    "acc = np.zeros(6)\n",
    "for i in range(6):\n",
    "    ind = i\n",
    "    diff = abs(preds[:,ind] - test_label[:, ind])\n",
    "\n",
    "    acc[i] = 1-(np.sum(diff))/2000\n",
    "\n",
    "print(acc)\n",
    "print(np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
