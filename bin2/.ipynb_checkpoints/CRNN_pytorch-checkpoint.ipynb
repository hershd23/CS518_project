{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "training_data_path = \"../data/image_data/training_data/\"    # define UCF-101 RGB data path\n",
    "validation_data_path = \"../data/image_data/validation_data/\"\n",
    "test_data_path = \"../data/image_data/test_data/\"\n",
    "save_model_path = \"./ResNetCRNN_ckpt/\"\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.25    # dropout probability\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 128\n",
    "\n",
    "# training parameters\n",
    "k = 6           # number of target category\n",
    "epochs = 50       # training epochs\n",
    "batch_size = 16\n",
    "learning_rate = 3e-4\n",
    "l_decay = 5e-4\n",
    "log_interval = 100  # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder, audio_model, final_net = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "    audio_model.train()\n",
    "    final_net.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, X_audio, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, X_audio, y = X.to(device), X_audio.to(device), y.to(device)\n",
    "        \n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        video_output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        audio_output = audio_model(X_audio)\n",
    "        comb_tensor = torch.cat((video_output, audio_output), 1)\n",
    "        output = final_net(comb_tensor)\n",
    "        \n",
    "        criterion = nn.L1Loss()\n",
    "        loss = criterion(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        #y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        #step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        #scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder, audio_model, final_net = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "    audio_model.eval()\n",
    "    final_net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (X, X_audio, y) in test_loader:\n",
    "            # distribute data to device\n",
    "            X, X_audio, y = X.to(device), X_audio.to(device), y.to(device)\n",
    "\n",
    "            video_output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            audio_output = audio_model(X_audio)\n",
    "            comb_tensor = torch.cat((video_output, audio_output), 1)\n",
    "            output = final_net(comb_tensor)\n",
    "\n",
    "            criterion = nn.L1Loss()\n",
    "            loss = criterion(output, y)\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            #all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    #all_y = torch.stack(all_y, dim=0)\n",
    "    #all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    #test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, \\n'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(audio_model.state_dict(), os.path.join(save_model_path, 'audio_model_epoch{}.pth'.format(epoch + 1)))\n",
    "    torch.save(final_net.state_dict(), os.path.join(save_model_path, 'final_net_epoch{}.pth'.format(epoch + 1)))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRNN_final_prediction(model, device, loader):\n",
    "    cnn_encoder, rnn_decoder, audio_model, final_net = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "    audio_model.eval()\n",
    "    final_net.eval()\n",
    "\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, X_audio, y) in enumerate(tqdm(loader)):\n",
    "            # distribute data to device\n",
    "            X, X_audio = X.to(device), X_audio.to(device)\n",
    "            video_output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            audio_output = audio_model(X_audio)\n",
    "            comb_tensor = torch.cat((video_output, audio_output), 1)\n",
    "            output = final_net(comb_tensor)\n",
    "            #y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n",
    "            all_y_pred.append(output)\n",
    "\n",
    "    return all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "params2 = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/image_data/pickle_files/y_5d_training_all.pkl', 'rb') as file:\n",
    "    train_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_training.pkl', 'rb') as file:\n",
    "    train_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_validation_all.pkl', 'rb') as file:\n",
    "    val_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_validation.pkl', 'rb') as file:\n",
    "    val_list = pickle.load(file)\n",
    "    \n",
    "with open('../data/image_data/pickle_files/y_5d_test_all.pkl', 'rb') as file:\n",
    "    test_label = pickle.load(file)\n",
    "\n",
    "with open('../data/image_data/pickle_files/vid_ids_5d_test.pkl', 'rb') as file:\n",
    "    test_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "training_df = pd.read_csv(\"../data/audio_data/pickle_files/training_df_all.csv\")\n",
    "validation_df = pd.read_csv(\"../data/audio_data/pickle_files/validation_df_all.csv\")\n",
    "test_df = pd.read_csv(\"../data/audio_data/pickle_files/test_df_all.csv\")\n",
    "save_audio_model_path = \"./audio_ckpt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "X_val = validation_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "X_test = test_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "Y_train = training_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]\n",
    "Y_val = validation_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]\n",
    "Y_test = test_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_val = Y_val.values\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_data = np.zeros((10000, 42))\n",
    "\n",
    "tot_data[0:6000] = X_train\n",
    "tot_data[6000:8000] = X_val\n",
    "tot_data[8000:10000] = X_test\n",
    "\n",
    "for i in range(6):\n",
    "    max_val = np.max(tot_data[:, i])\n",
    "    min_val = np.min(tot_data[:, i])\n",
    "    tot_data[:, i] -= min_val\n",
    "    tot_data[:, i] /= (max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tot_data[0:6000]\n",
    "X_val = tot_data[6000:8000]\n",
    "X_test = tot_data[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_audio_train = torch.tensor(X_train).float()\n",
    "X_audio_val = torch.tensor(X_val).float()\n",
    "X_audio_test = torch.tensor(X_test).float()\n",
    "Y_audio_train = torch.tensor(Y_train).float()\n",
    "Y_audio_val = torch.tensor(Y_val).float()\n",
    "Y_audio_test = torch.tensor(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = Audio_Model(drop_p=dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = list(audio_model.fc1.parameters()) + list(audio_model.fc2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = Dataset_CRNN(training_data_path, train_list, train_label, selected_frames, X_audio_train, transform=transform), \\\n",
    "                       Dataset_CRNN(validation_data_path, val_list, val_label, selected_frames, X_audio_val, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_CRNN(test_data_path, test_list, test_label, selected_frames, X_audio_test, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_net = Final_Net(drop_p = dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "    rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.module.fc1.parameters()) + list(cnn_encoder.module.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc2.parameters()) + list(cnn_encoder.module.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters()) + list(audio_model.parameters()) + list(final_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    if __name__ == '__main__':\n",
    "        train_losses, train_scores = train(log_interval, [cnn_encoder, rnn_decoder, audio_model, final_net], device, train_loader, optimizer, epoch)\n",
    "        epoch_test_loss = validation([cnn_encoder, rnn_decoder, audio_model, final_net], device, optimizer, test_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    np.save('./CRNN_epoch_training_lossesMAE.npy', A)\n",
    "    np.save('./CRNN_epoch_test_lossMAE.npy', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_3DCNN(test_data_path, test_list, test_label, selected_frames, X_audio_test, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'cnn_encoder_epoch20.pth')))\n",
    "rnn_decoder.load_state_dict(torch.load(os.path.join(save_model_path, 'rnn_decoder_epoch20.pth')))\n",
    "audio_model.load_state_dict(torch.load(os.path.join(save_model_path, 'audio_model_epoch20.pth')))\n",
    "final_net.load_state_dict(torch.load(os.path.join(save_model_path, 'final_net_epoch20.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_pred = CRNN_final_prediction([cnn_encoder, rnn_decoder, audio_model, final_net], device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((2000,6))\n",
    "k = 0\n",
    "for i in range(len(all_y_pred)):\n",
    "    for j in range(len(all_y_pred[i])):\n",
    "        batch_pred = all_y_pred[i].cpu()\n",
    "        preds[k] = batch_pred[j]\n",
    "        k +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Metrics IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros(6)\n",
    "for i in range(6):\n",
    "    ind = i\n",
    "    diff = abs(preds[:,ind] - test_label[:, ind])\n",
    "\n",
    "    acc[i] = 1-(np.sum(diff))/2000\n",
    "\n",
    "print(acc)\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "\n",
    "diff = abs(preds[:,ind] - test_label[:, ind])\n",
    "avg = np.mean(test_label[:, ind])\n",
    "print(avg)\n",
    "avg_diff = abs(test_label[:, ind] - avg)\n",
    "\n",
    "acc = 1 - ((np.sum(diff))/np.sum(avg_diff))/2000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEPENDENT AUDIO MODEL TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "training_df = pd.read_csv(\"../data/audio_data/pickle_files/training_df_all.csv\")\n",
    "validation_df = pd.read_csv(\"../data/audio_data/pickle_files/validation_df_all.csv\")\n",
    "test_df = pd.read_csv(\"../data/audio_data/pickle_files/test_df_all.csv\")\n",
    "save_audio_model_path = \"./audio_ckpt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lb = test_df['video_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    if(audio_lb[i] != test_list[i]):\n",
    "        print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "X_val = validation_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "X_test = test_df.drop(['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness', 'video_id'], axis=1)\n",
    "Y_train = training_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]\n",
    "Y_val = validation_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]\n",
    "Y_test = test_df[['interview_score','extraversion','agreeableness','conscientiousness','neuroticism','openness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_val = Y_val.values\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_data = np.zeros((10000, 42))\n",
    "\n",
    "tot_data[0:6000] = X_train\n",
    "tot_data[6000:8000] = X_val\n",
    "tot_data[8000:10000] = X_test\n",
    "\n",
    "for i in range(6):\n",
    "    max_val = np.max(tot_data[:, i])\n",
    "    min_val = np.min(tot_data[:, i])\n",
    "    tot_data[:, i] -= min_val\n",
    "    tot_data[:, i] /= (max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tot_data[0:6000]\n",
    "X_val = tot_data[6000:8000]\n",
    "X_test = tot_data[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_audio_train = torch.tensor(X_train, device=device).float()\n",
    "X_audio_val = torch.tensor(X_val, device=device).float()\n",
    "X_audio_test = torch.tensor(X_test, device=device).float()\n",
    "Y_audio_train = torch.tensor(Y_train, device=device).float()\n",
    "Y_audio_val = torch.tensor(Y_val, device=device).float()\n",
    "Y_audio_test = torch.tensor(Y_test, device=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = Audio_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = list(audio_model.fc1.parameters()) + list(audio_model.fc2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_audio = torch.optim.Adam(audio_params, lr=1e-3)\n",
    "audio_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(audio_epochs):\n",
    "    audio_model.train()\n",
    "    optimizer_audio.zero_grad()\n",
    "    train_ops = audio_model(X_train)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = torch.sqrt(criterion(train_ops, Y_train))\n",
    "    loss.backward()\n",
    "    optimizer_audio.step()\n",
    "    \n",
    "    print('Train Epoch {}\\tLoss: {:.6f}' .format(epoch+1, loss))\n",
    "    \n",
    "    audio_model.eval()\n",
    "    val_ops = audio_model(X_val)\n",
    "    val_loss = torch.sqrt(criterion(val_ops, Y_val))\n",
    "    \n",
    "    print('Val loss {:.6f}' .format(val_loss))\n",
    "    \n",
    "    torch.save(audio_model.state_dict(), os.path.join(save_audio_model_path, 'audio_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.load_state_dict(torch.load(os.path.join(save_audio_model_path, 'audio_epoch100.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.eval()\n",
    "test_ops = audio_model(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
